#!/bin/tcsh 
#  This script will run HOMME using the NGGPS benchmark problem with 128 levels and 10 tracers.
#BSUB -P CLI115
#BSUB -W 10
#BSUB -nnodes 1
#BSUB -alloc_flags gpumps
#BSUB -J dycore_acc
#BSUB -o dycore_acc.%J
#BSUB -e dycore_acc.%J

source $MODULESHOME/init/bash
module purge
module load DefApps
module load pgi/18.7
module load netcdf
module load netcdf-fortran
module load parallel-netcdf
module load cmake
module load cuda
module load hdf5

module list

limit stacksize unlimited
limit coredumpsize unlimited

#  set paths to source code, build directory and run directory
set wdir =  /gpfs/alpine/cli115/proj-shared/$USER/homme-runs/acc           # run directory
set HOMME = `pwd`/../../..               # /path/to/acme/components/homme
set MACH = $HOMME/cmake/machineFiles/summit.cmake
set exe = $HOMME/compile_scripts/summit/build_summit/src/theta-l-acc/theta-l-acc

#  Which problem?  tiny, ne30 or ne120 configuration
set namelist = nggps-tiny.nl ; set name = tiny      # use 4 nodes
    
#  mpi run command
setenv OMP_NUM_THREADS 2
setenv OMP_STACKSIZE   64M
setenv PER_NODE        18
# compute number of MPI tasks
set NNODES=`echo $LSB_HOSTS | tr " " "\n" | uniq -c | wc -l`
@ NNODES -= 1
set NMPI = $NNODES
@ NMPI *= $PER_NODE
@ NMPI /= $OMP_NUM_THREADS

echo NODES =            $NNODES
echo NMPI_PER_NODE =    $PER_NODE
echo NTHREADS_PER_MPI = $OMP_NUM_THREADS
# note: in tests on 4K nodes,the --bcase and --compress options were much slower. DONT USE:
set mpirun = "$HOMME/test/benchmarks/NGGPS/mpirun.summit -n $NMPI -N $PER_NODE -gpu"
echo "mpi commnand: $mpirun"

set input = $HOMME/test/benchmarks/NGGPS  # input files for test case
set vdir = $HOMME/test/vcoord             # vertical coordinate files
set run = $wdir/$name

#  Run the code
mkdir -p $run/movies
cd $run
# copy all vertical levels to run directory
rsync -a  $vdir/sab?-128.ascii  $run   
# namelist has to be called input.nl for perf settings to be read
rm -f input.nl
cp -f $input/$namelist input.nl

date
$mpirun $exe < input.nl
date

#Print timing data for convenience
if (-f  HommeTime  ) then
   # save timings from run
   set timingfile = nodes${NNODES}.tasks${PER_NODE}.threads${OMP_NUM_THREADS}.HommeTime
   set summary    = nodes${NNODES}.tasks${PER_NODE}.threads${OMP_NUM_THREADS}.summary
   mv HommeTime $timingfile
   # total run time (not counting init)
   grep -a prim_main_loop $timingfile | head -1 | tee $summary
   # breakdown dyn, tracers, remap.  about 97% of the cost:
   grep -a prim_step_dyn  $timingfile | head -1 | tee -a $summary
   grep -a PAT_remap      $timingfile | head -1 | tee -a $summary
   grep -a vertical_remap $timingfile | head -1 | tee -a $summary
   echo "run parameters:" >> $summary
   cat input.nl >> $summary
endif

