@ Review what gcami vs. gcamiold in the gcam_setdensity_mod() section, line
  447 - 490 in iac_comp_mod.F90.

5/10/2019

* Hidden away in gcam/src/iac/shr is iac_fields_mod.F90, with some useful
  definitions.  I'm not sure, but it seems like a "shr" directory is
  designed to be shared between various elements, but since I mostly need
  this stuff internal I'm going to move it into the coupling directory.

* Anyway, in iac_field_mods, we have this defined:

    integer, parameter, public :: iac_glm_nx  = 720
    integer, parameter, public :: iac_glm_ny  = 360

  So now this clicks into place - we use two grids, basically an input and
  an output grid - clm_nx,clm_ny matches the land grid, and is used on
  input clm.  iac_glm_nx,iac_glm_ny defines the glm grid, which is 720x360,
  the one used by updateannuallanduse(), and the output of the glm code.

@ So this means we probably *don't* have to transform the input data, and
  can define the iac grid to match the clm grid.  But it means we *do* have
  to transform the glm grid back to the iac/clm/cam grid.

* It seems like iESM has a somewhat different EClock interface, and
  probably a cdata interface as well.  For EClock, at least, rather than
  hunt down all the uses of EClock inside the gcam wrapper code I'm just
  going to define a GClock just like the gcam-Eclock, and translate to and
  from E3SM EClock.  (From now on, EClock means what E3SM says, and GClock
  is the other thing.)  It's possible the iESM calls are compatible with
  EClock, but redundancy like this isn't the worst thing in the world.

  GClock basically just has elements for holding the various alarms to run
  things, which are moot in my implementation, and then ymd, tod, and dt.
  Still, there we go.

* Back to iac2gcam_run_mod(), anything that cracks clm_base
  (iac_base_clmfile) should stay  that way - it's basically a configuration
  file.  What I need to figure  out now is how we use those values when we
  aren't doing the goofy  interpolate stuff.

* iac2gcam_run_mod() does do a lot of calculations aside from just cracking
  the file and calling calc_clmC().  But calc_clmC() mostly cracks and uses
  the clm history file, which should be replaced with the lnd2iac_vars that
  we've imported from the coupler.  THUS, I now need to go back to
  iac_import() and figure out how to turn an AVect into useable variables
  in calc_clmC().

  Another thing calc_clmC() does is read/write restarts, and writes out
  an iac history file, both of which it should keep doing.

  The final thing calc_clmC() does, and where it gets it's name, is turn
  this analysis of monthly clm files into the weird "5-year mean of yearly
  maximums" fields.  My initial plan was to simply have the MCT aggregator
  generate whatever yearly means we need, but if it's a complicated and
  weird calculation like that, then...eh, it's probably still best.

! Okay, so the above paragraph clarified something: here is what
  we end up using, I think, in gcam, for npp and hr:

  The five year *mean* of the yearly *max* of the monthly *averages*. 

  So, that's a mean of the maximum of means.  Yeesh.

5/9/2019

* I've been banging my head again on iac2gcam_mod.F90 again, the function
  iac2gcam_run_mod().  This is the key function that includes what will
  become the l->z coupling, and, right now, reads the clm history file.  I
  need to make some notes here about how this is structured and what it
  does, so I can figure out what parts I keep and what I dont. 

? The big question mark for me is how the time scales work.  This is the
  time scale for how things run in iESM (see logicals set at c. line 441 in
  iac_comp_mod.F90).  This is the order they are called in the code, so
  when these time scales match up they will happen in this order.

1 Every *month*, iac_run_mod() calls iac2gcam_run_mod() to run calc_clmC(),
  which reads the clm h1 (monthly) history file, and does some stuff with
  it - at the very least, it returns npp and hr, but it might do some
  additional calculation or accumulation. 

  Okay, calc_clmC() file basically accumulates inputs and writes history
  and restart files.  See below...

2 Every *five years*, on January 1, this iac2gcam_run_mod() call goes on to
  do "calc_avg" section of the code, and then iac_run_mod() does some stuff
  in the AGcamsetden section.  ("long_gcam_timestep").

  It looks this AGcamsetden section is actually really important, because
  the call to gcam_setdensity_mod() is how we set the carbon data for the
  defined year.  I need to review gcami vs. gcamiold, but we seem to be
  doing yearly interpolations of gcami over the next five years, using the
  previous five years of clm.h1 data that we accumulate in calc_clmC().

3 Every *year* (one year), iac_run_mod() calls gcam_run_mod(), apparently by
  advancing the model year by 5 years.  (There's some stuff if the
  long_gcam_mod is set to 15 by setting sneakermode, but I'm going to
  ignore that for now).

4 Every *five years*, iac_run_mod() will then call gcam2emisfile_run_mod(),
  to send CO2 emissions to the atm model.

5 Every *year*, iac_run_mod() will then call gcam2glm_run_mod(),
  glm_run_mod(), and glm2iac_run_mod().

6 Finally, iac_run_mod() dumps a history file

--------------

* So, what does this mean?  First of all, we will now run
  iac2gcam_run_mod(), and/or calc_clmC() (or some other function to do
  those things) not every month, but every regular gcam time step (1 or 5
  years).  I think we *also* will do a lot of what the calc_avg section
  does, since it really is some additional calculations that get sent to
  the gcam subcomponent, but is also some averaging.  All that averaging
  stuff should be handled by mct, now, so when we run iac_run_mct() the
  input AVects should already be averaged over the year.

* I need to go back and revisit the model times we are running here.  I
  *believe* what we end up doing for year yt, we run gcam at yt+5.  We do
  that every year, I guess so that we have a running list if inputs to send
  back to clm every year?  That doesn't seem quite right, so I need to go
  back and track what the EClock(iac_Eclock_ymd) gets set to before calling
  all these functions.

* So, my initial guess is that we appear to be reading and accumulating
  data on a monthly/5-year time scale - but we *run* gcam every year for
  five years in the *future*.  Hmm.

* Quick note - from the restart file in Kate's sample run,
  (iac_clmC_file.r.2090-11.nc), we can see that the "iac_iac_nx,
  iac_iac_ny" we are using in calc_clmC() is the same as clm_nx and clm_ny
  from the namelist.  (clm_nx=288, clm_ny=192).  So, the iac input grid is
  the same as the clm grid - good to know.  (We must convert to the glm
  output/updatelanduse() grid of 720x360 later on).

* A review of calc_clmC() shows that it does this:  reads clm.h1 file on a
  monthly basis and accumulates the *yearly maxes* of these inputs, and
  then every five years computes the *average* of those yearly maxes, and
  sends that back out and writes a history file.  (It also manages some
  restart read/write stuff.)

* Back to iac_comp_mod.F90, then - after doing calc_avg inside of
  calc_clmC(), we then interpolate the gcami over the next five years and
  push that carbon data to gcam for those years via gcam_setdensity_mod().
  Whew! 

5/6/2019

* So, I need to rename some of my stuff.  I've been using "iac" to mean the
  generic coupled component, and "gcam" the specific model; the iESM code
  base uses "iac" to mean the specific coupled component, with "gcam" (and
  "glm", etc.) as a subcomponent of the overall thing.

  When I say "iac" I mean it in the context of MCT, but iESM uses it as a
  wrapper for the fortran interface coming out of the (gcam,glm,...)
  submodels. 

  I can't avoid using iac now, and I don't want ot rename the iESM files,
  so we'll just deal with that: try to keep iac_ as a prefix for files that
  are seen outside of (gcam,glm), and understand that iac2x or x2iac means
  interaction with the submodel.

! But, I can't call my functions and modules "gcam_", because that's now
  means the submodule, the interface with the C code a layer down, not the
  interface with the coupler at this layer.  So, specifically, I can't have
  gcam_run(), gcam_init(), gcam_var_set(), which I've stubbed into
  iac_comp_mct.F90.  Instead, I need a new tag to indicate "internal
  (gcam,glm,...) group layer functions to interact with mct() rather than
  submodules".

  Maybe "giac" - for gcam/iac layer?  That would actually be a better tag
  for what iESM calls iac, but that ship has sailed.  Maybe I could go
  ahead and rename iac_comp_mod.F90 into giac_comp_mod.F90.

5/5/2019

* Jeez, I keep losing track of the calling tree inside of iESM, which I'm
  trying to lift as much as possible over to E3SM.  Fortunately, I did make
  a map of it - so, to remember it, this is the way the tree should work in
  E3SM:

   driver ->
   iac_run_mct() [ gcam/src/cpl/iac_comp_mct.F90 ]  [[ iESM: lnd_run_mct() ]]
XXXX
?  -> gcam_run() ? [ see below - do we need gcam_run(), or will
      iac_run_mct() do everything?
XXXX
   -> iac_run_mod()         [ gcam/src/iac/coupling/iac_comp_mod.F90 ]
    ->   iac2gcam_run_mod() [ gcam/src/iac/coupling/iac2gcam_mod.F90 ]
    ->   gcam_run_mod()     [ gcam/src/iac/coupling/gcam_comp_mod.F90 ]
  ( ->   gcam2emissfile_run_mod() [ gcam/src/iac/coupling/gcam2emisfile_mod.F90 ] )
    ->   gcam2glm_run_mod() [ gcam/src/iac/coupling/gcam2glm_mod.F90 ]
    ->   glm_run_mod()      [ gcam/src/iac/glm/glm_comp_mod.F90 ]
    ->   glm2iac_run_mod()  [ gcam/src/iac/coupling/glm2iac_mod.F90 ]
         -> updateannuallanduse()

* Okay, so this is kind of a nightmare to keep track of all this.  A lot of
  this is to have fortran calls to connect better with the coupler and
  other models.  But there's a lot of ambiguity when we have both iac and
  gcam floating around - I think that logically they considered "iac" a
  shell around (gcam,glm, and emissivity) subcomponents, and that's why we
  do stuff like iac2gcam and glm2iac.

  So, hopefully, this means I can mostly just set things up in
  iac_run_mct() to do this calling sequence, and I'll be good.  But once
  again I need to review what all these code elements are doing - there is
  E3SM-like stuff Eclock and attribute vectors inside the iac-named routines
  inside of gcam/src/iac, and probably elsewhere.  

* Okay, where else am I?  I need to write these these modules:

  gcam_var - just public variables like gcam_lon, gca_lat, gcam_Active stuff like that
  gcam_mod - gcam_init() and gcam_run()
  iac_ctl_mod  - the iac_ctl control structure, which I don't completely
    understand. It's called rtmCTL in rof (mosart), and seems to be just a
    structure to hold a lot of information taht you pass back and forth.  I'm
    trying to find the analog in lnd - clm seems to store the boundary
    information itself in a separate bounds structure. 

    For now I'm goign to put the big ctl everywhere so I can identify
    it easily, in case I search differently.

* It's hard to track down all the things we need to configure when
  everybody has a different idea of what those things are, and how they
  should be organized.  

* So iac_init_mct() is (roughly) finished.  Here's the next things to do:

@ gcam_init() - big one, figure out how to make grids, etc.
@ iac_run_mct() - figure out what this does - does it the (gcam,glm,emiss)
  functions as above, or does it call gcam_run(), which does all thato?

  See, the thing is I think the iac_run_mct() function is basically for
  coupling.  I don't know if it should do the model flow, or if it should
  farm that out to a separate function.  Again, question of organization -
  what I'm trying to do is separate all this E3SM stuff from the actual
  model stuff.  That may be handled appropriately just by these fortran
  functions inside the coupling directory.

  I'm tempted to rename them all, because it's all very confusing, but
  probably not - just remember that we are kind of treating gcam,glm,emiss
  as subcomponents of iac, which is why we have functions to go back and
  forth. 
  
5/1/2019

* From Gautam: 

  The landuse data is available on Anvil at

  /lcrc/group/acme/public_html/inputdata/lnd/clm2/surfdata_map/landuse.timeseries_ne30np4_ssp5_rcp8.5_simyr2015-2100_c190411.nc

* Phone call today - talked about gridding, and the transformations between
  lnd->gcam, gcam->glm, glm->lnd.  Here is my understanding:

  The glm internal grid is hardcoded to 360x720. In iESM, we took lnd->gcam
  regional somehow, but lnd was at 1 degree, which is not the hardcoded
  360x720 glm grid.  So there must have been some interface to read at lnd
  resolution, convert to gcam regions, then convert onto the glm grid, and
  finally write out the dynamic crop file.  Then clm would read that file
  and convert to the resolution it needed.

* Thus, for E3SM/GCAM, we need three conversions:

1 lnd->gcam
2 gcam->glm
3 glm->lnd

  Internally, E3SM and MCT accomplish this simply by linear algebra: if
  you've got inputs x need outputs y, each on their own grid, define
  a mapping T from the x grid to the y grid and then simply run y=Tx for
  all x.  This interpolation is almost by definition a very sparse matrix,
  since the only thing that contributes to y[i] are the points x[j] that
  are around i, so there are sparse matrix multiplier tools that make this
  go fast (and, I think, in a distributed manner).  One of the MCT papers
  talks about this.

  I believe that (2), gcam->glm, happens internally as part of the gcam,glm
  calling sequence - gcam works on its regions, and then glm scales it to
  the grid it needs to work on, somehow.  So that leaves defining
  transformation mappings for (1) and (3), and doing the regrid
?  transformation somewhere in the processing flow.  Logically, this might
  mean we run y[gcam]=Tx[lnd] whenever gcam gets lnd vars from the coupler,
  and similarly when lnd reads the glm stuff.  But it could mean we
  transform *before* sending to the coupler...

4/30/2019

* I've been rereading and working through the MCT papers again, because I'm
  trying to understand the gsMap thing.  I think it tells everybody what
  segment of global ids is on which MPI process, so that communication when
  you want to find those global ids is possible.  Thus, even for Gcam,
  running on nproc=1, it needs a gsmap so that lnd and atm can find the
  things they need from the coupler.

  At least, I *think* that's what's going on.  I'm still a little unclear
  on how it's actually used in practice - is this how we interpolate from
  one grid to the other?  Also, how we map from one domain decomposition to
  another...
  
  I've spent a lot of time on this, the *theory* of how the coupler works,
  and so I'm getting behind on actually implementing this stuff, so maybe I
  should table the reread and start pounding some code again.  But this is
  important to understand.

* Ah, at last - the GeneralGrid class is how we define the actual grids we
  use in our attribute vectors - "a literal listing of each mesh point's
  coordinates and geometric attributes".  So, somehow in the initializating
  we need to use this to describe how our GCAM mesh works.

* Note that GCAM's "grid" only makes sense in the interface with lnd and
  atm - internally it has a region based location description, and it's
  only in the sense that it gets input from and output to other components
  (and files and stuff) that we need to deal with a grid.  Nevertheless,
  we'll define our grid by the pftdyn/surfdat file, and use MCT to interact
  with other components.

4/23/2019

* Okay, I was wrong - MAXINPIX and MAXINLIN are not actually used in
  updateannuallanduse.c.  It looks like the in and out arrays are all on
  teh same grid - dimensioned by MAXOUTPIX*MAXOUTLIN.  

  So, it seems clear that this version of GCAM was designed to run on the
  720x360 grid, only.

  Also, I'm pretty sure a lot of the input arrays are not actually used,
  although things like incurrentpftval[][] is read in from the surfdat
  file. 

* Yeesh, I'm still having trouble figuring out the grid here, it all seems
  set elsewhere and assumed.  At any rate, plodata[][] is clearly on a
? 360x720 grid, hardcoded.  Could we simply change the values of MAXOUTPIX
  and MAXOUTLIN and have this code work, assuming we had a valid
  surfdat/mcrop file for the new resolution? 

! Okay, now I really have no idea what gets called and what doesn't inside
  updateannuallanduse_main().  It looks like they crack some netCDF files,
  maybe with dynamic values, maybe written out by glm?

  Okay, so all this stuff seems to be on the same grid, which suggests, and
  this might be crazy, that this whole gcam development tree can *only* run
  at 360x720 gridcell resolution.  Could that be true?  I just don't see
  how anything in this file would work if you changed the grid.

* So, what would this imply then - GCAM itself works regionally, and then
  Kate has mentioned something about downscaling.  So maybe we just say the
  GCAM grid is always this 360x720 one, and then, maybe? MCT can do the
  interpolation and stuff onto whatever the land grid is?

? Stray thought...those "mapping" functions that befuddeled me when I first
  started all this archeology - those might be how you interpolate from one
  grid to the next.  Nb. my comments about prep_lnd_get_mapper_Sa2l() from
  6/27/18 or prep_lnd_calc_r2x_lx() from 1/5/18.

  Could something like this be used to go from our require 360x720
  resolution to whatever we have for lnd?

---------------

* Okay, for now I'm going to let this percolate in background and go back
  to trying to get all the framework stuff done to actually get a gcam
  component running.   I'm in the middle of setting up iac_mct_mod.F90,
  which contains the functions the driver calls.  From there, we need to
  modify iac_run_mod.F90, which is the fortran interface to calling GCAM
  with all the right bits and pieces.

* I've stubbed in a gcam_init() function, but I'm looking at init functions
  for other components to see what is going on in there.  Typically, the
  comments suggest that's where you read in the grid and namelist
  information.  The lnd init was pretty complicated, so I'm reviewing
  Rtmini() from mosart (I know I've done this before), just to get an idea
  of what kinds of setup you expect to need.  Of course, I am especially
  interestd in anything having to do with the component grid, since I have
  yet to figure out how that figures into things.

* Well, Rtmini() is ~1000 lines long.  So much for simple.

  My hope is that most of this model based init has already been written by
  the gcam functions.

  Anyway, here is what Rtmini() does:

* namelist /mosart_inparm/ so, there's that.
* Read in namelist in masterproc, then mpi_bcast() to other nodes with
  values.  That make sense.
* Some logging if (masterproc) - write(iulog,*)...
* Switch on do_rtm/rtm_active and a flood_active logical
* ...error checking, logging
* Time manager init - ncd_pio_init(), check for restart, then
  timemgr_init() if starting or custom RtmRestTimeManager() if restarting,
  I think.  Also - here is where we read and set dtime_in - the coupling
  period. 
* Initialize rtm_trstr, for tracers - I think it's just a string saying
  what they are, colon separated so maybe it's a formatted list of tracers
  or something.
* Ah, here is where we set the grid.  ncd_pio_openfile() on the
  "frivinp_rtm" namelist option, which is a filename.  Crack that file,
  read in "rtmlon" and "rtmlat", the dimensions of the grid.  Then allocate
  stuff - first lat and lon based, then flattened 1D arrays.  Then read
  "longxy" and "latixy" to fill in rlon[] and rlat[] and others in control
  structures and whatever.  Also read "area" and "ID" to fill in other
  arrays.

  There's a clue in here - the area_global[g] array is flattened 1D from
  [lon][lat] - the area array from the netcdf file is 2D, so we build a
  flattened index n=(j-1)*rtmlon+i, for i over lat, j over lon.  So,
  obviously, this is a normal column major linearized index calc.

! So, we actually *crack a file* to figure out our grid!  This makes sense
  - you set which file you want to use in the namelist and from there
  extract the grid.

! So, the mozart file has something called dnID, which is then set to a 1D
  array dnID_globa().  But, then we have this comment (c.470 in RtmMod.F90)

===================================
    !-------------------------------------------------------
    ! RESET dnID indices based on ID0
    ! rename the dnID values to be consistent with global grid indexing.
    ! where 1 = lower left of grid and rtmlon*rtmlat is upper right.
    ! ID0 is the "key", modify dnID based on that.  keep the IDkey around
    ! for as long as needed.  This is a key that translates the ID0 value
    ! to the gindex value.  compute the key, then apply the key to dnID_global.
    ! As part of this, check that each value of ID0 is unique and within
    ! the range of 1 to rtmlon*rtmlat.
    !-------------------------------------------------------
===================================

  Okay, so there we go - global grid indexing is 1,lon*lat, starting at (lon,lat)
  (-180,-90) and going up to (180,90).  I'm going to have to draw this out,
  because my row-major instincts will get it backwards, but I'm pretty sure
  we vary lon fastest (that's why n=j*nlon+i).

  Anyway, it looks like they build this IDkey() array that takes the 'ID'
  field for each grid cell (read from the file) and assigns it the global
  grid index going from 1,lon*lat.  This isn't important for me - they just
  have a different ID for grid cells, so this is how you go back and
  forth.  So we do that with dnID_global(), so it is now ordered by global
  index rather than 

* Back to Rtmini:

* Calculated edges of grid cell - why do this?
* Grid mask - land, ocean, ocean outlet from land.  I *totally* do not
  understand what is going on in the loops at c. line 574, but whatever,
  figuring out which grid points are what kind.  It has something to do
  with the dnID_global() we read from the file, so whatever.  

* From here, calculate various values having to do with the grid, which is
  very rtm specific.  Basic idea: find all the river basins, and then
  allocate them to pes.  That makes sense - you parallelize river models by
  distributing the river basins around.  Anyway, the idea is the init
  function also figures out how to distribute it's calculations.  Then,
  assign cells to different chunks accordingly. 

  Basically, this sets rtmCTL%begr and rtmCTL%endr for each processor
  running this init, and thus you allocate arrays accordingly.  Then do
  some more initialization based on the decomposition.

* Then, at the end, they do stuff like run nr over rtmCTL%begr,rtmCTL%endr,
  and set rtmCTL%lonc(nr) and rtmCTL%latc(nr) accordingly.  So, this is how
  you build your grid - but it's not really global, is it?  Maybe it is for
  anybody attempting to work with rtm data, include rtmMod and here's the
  array.  But even then, it's pe dependent - rtmCTL clearly is different
  for each processor, whcih is why rtmCTL%begr,rtmCTL%endr works, so for
  the moment we've build a local set of lonc and latc values for each grid
  cell nr.  How do we use that grid external to rtm - i.e. how do we couple
  with this information?

  I *think* the answer might be in the functions we call right after
  Rtmini() in rof_comp_mct.F90 - rof_SetgsMap_mct().  Similarly, lnd does
  the same thing - call lnd_SetgsMap_mct().  Okay, so my guess is that is
  the function that somehow informs MCT of how all these x2z and z2x
  attribute vectors are dimensioned.

@ iac_SetgsMap_mct() - figure out how to do this.

* But, back to Rtmini(): line 1051 calls mct_gsMap_init() as part of
  "Compute Sparse Matrix for downstream advection".  Also, calls to
  mct_sMat_init(), which, I'm at a loss as to what they do.  sMat obviously
  means "sparse matrix", so some kind of linear algebra re-decomposition?

  Whew, a lot of stuff I don't get - using MCT to, I think, do some
  distributed linear algebra, which is why they need to stuff things into
  attribute vectors apparently outside of any "coupling" considerations.  I
  guess anything you distribute can use MCT functionality?  

@ Review lnd() initialization and see if it does somethign like this.

* Okay, now back to something I understnd - line 1328, if restart crack and
  read restart file.

* Initialize history handler and fields.

* That's it.

4/22/2019

* Just a reminder - the gcam submodule is in:

    /home/shippert/E3SM_active_gcam/components/gcam/src

  This means: 

    /home/shippert/E3SM_active_gcam/components/gcam/cime_config
  
  ...will be part of the main E3SM branch.

  So, to commit and push to the gcam submodule, go to gcam/src and do your
  thing.  

  Apparently, to connect this version of gcam with a version of E3SM, you
  go up to E3SM and add .../gcam/src, commit, and push - it will then
  codify into the E3SM commit which submodule commit you are using, so it
  knows which commit tot grab from the submodule repository.

4/22/2019

* The function updateannuallanduse.c returns its output into
  plodata[][pft+7] - look at copy2plodata(); it fills in the
  plodata[outgrid][pft] from outhurtpftval[pft][outgrid] (you have to flip
  indeces because C->f90 row vs. column major order), then adds in some
  additional outhurtt###[][] stuff on top of that.  So all that stuff is
  used for gcam/glm.

  Anyway, this means I don't have to muck with updateannuallanduse.c at all
  - it *already* returns what I want in plodata[g][pft].  Just a straight
  copy into the already allocated avects!

! Except, the plodata[][] is on a hardcoded grid of MAXOUTPIX=720 and
  MAXOUTLIN=360.  

XXXXXXXXXXXXXXXXXX see 4/23/2019
  I think this is the "downscaling" Kate has talked about,
  since MAXINPIX=7200 and MAXINLIN=3600.
XXXXXXXXXXXXXXXXXX

  ...so it might be that I either make MAXOUTPIX and MAXOUTLIN dynamic,
  which *would* require refactoring of updateannuallanduse(), or we have to
  do some kind of transformation ourselves explicitly to put everything on
  the land grid. 

* For now, don't worry about that - instead, it's finally time to start
  pounding the iac_run_mct() function, to call the iac_run_mod() functions
  that do the GCAM calling routine, and just make sure we put in hooks to
  push our output back to the avect as we can.

4/17/2019

* Okay, well, I got it all wrong. The *only* thing that changes in the
  mcrop_dyn.nc file that we read is PCT_PFT.  All the PCT_WETLAND,
  PCT_URBAN, etc. stuff is static, and the same from run to run.  We aren't
  actually modifying the land!  We are simply tracking how much of the land
  has vegetation on it - crops and deforestation, I think.

  Because it is the only field that changes, we *only* need to extract and
  couple with PCT_PFT!  In mcrop_dyn.nc, PCT_PFT has the dimensions:

  	float PCT_PFT(time, lsmpft, lsmlat, lsmlon) ;

  So: PCT_PFT[time][pft][lat][lon] - it's an array of percentages of each
  of the 17 pfts at each time and location.

  That's what is goign on in writepftdynfile() - it
  first tracks down the "nvarspcnt" index (probably the "percent" vars),
  finds PCT_PFT, appends the current time stamp, then goes over each grid
  and each pft and stores the 17 values - it looks like it's trying to
  write it out in flattened notation, which maybe is how you do straight
  netcdf, I can't remember anymore.

  Okay, so the values[] array at line 3038 is simply size 17 for the 17
  pfts (17th is unused).

  So this means that: outhurttpftval[pft][lat,lon] is exactly PCT_PFT, and
  I simply need to bring that out and stuff into an AVECT.

? I *think* that updateannuallanduse.c *hardcodes* the grid that the mcrop
  file uses, so that suggests we have one mcrop file with its own grid, and
  thus we need to transform this crop data onto whatever grid clm is
  running on.  But, clm probably already does that!  So, what grid do I use
  to transmit this out, something else?

@ Also, fix seq_flds_mod.F90 - I'm not sending that PCT_WETLAND stuff.

4/16/2019

* Back to clm reading surfdat file - it looks like the routin
  surfrd_get_data() in srfrdMod.F90 is what we want.  This is in the iESM
  tree, so who knows if current clm's do the same thing, but whatever, I'm
  following this rabbit hole as far as it goes, at least to figure out what
  it pulls out of the file.  

  Also, maybe in srfrd_wtxy_special(), and surfrd_wtxy_veg_all(), and
  surfrd_wtxy_veg_dgvm()? 

  A bunch of functions called by srfrd_get_data(), which seem to read from
  the cracked file, which have something to do with calculating the
  "weights". 

  Also, then, srfrd_get_grid() and srfrd_get_globmask().  Whew, okay, a lot
  to unpack here - so, grid is about the coordinate system and globmask is
  about (I think) how much land on each cell.  Jeez, this is a mess.

! Okay, so, here's a clearinghouse of every single field I think we are
  trying to read from the surfdat file.  I'm going to just list them here,
  and then try to match them with what I'm seeing in the test iESM run file
  listed below (way below), and see if they match up.  If so, then let me
  track backwards and see where the GCAM stuff modifies those things, and
  *then*, finally, I'll have a list of what things need to be z->l
  coupled.   

  From: egrep 'subroutine|ncd_io' surfrdMod.F90, then cleaned up a bit:
===================================
  subroutine surfrd_get_globmask(filename, mask, ni, nj)
       call ncd_io(ncid=ncid, varname='LANDMASK', data=idata2d, flag='read', readvar=readvar)
          call ncd_io(ncid=ncid, varname='mask', data=idata2d, flag='read', readvar=readvar)
       call ncd_io(ncid=ncid, varname='LANDMASK', data=mask, flag='read', readvar=readvar)
          call ncd_io(ncid=ncid, varname='mask', data=mask, flag='read', readvar=readvar)

  subroutine surfrd_get_grid(ldomain, filename, glcfilename)
       call ncd_io(ncid=ncid, varname= 'area', flag='read', data=ldomain%area, &
       call ncd_io(ncid=ncid, varname= 'xc', flag='read', data=ldomain%lonc, &
       call ncd_io(ncid=ncid, varname= 'yc', flag='read', data=ldomain%latc, &
X      call ncd_io(ncid=ncid, varname= 'AREA', flag='read', data=ldomain%area, &
X      call ncd_io(ncid=ncid, varname= 'LONGXY', flag='read', data=ldomain%lonc, &
X      call ncd_io(ncid=ncid, varname= 'LATIXY', flag='read', data=ldomain%latc, &
       call ncd_io(ncid=ncid, varname=trim(vname), data=rdata2d, flag='read', readvar=readvar)
       call ncd_io(ncid=ncid, varname=trim(vname), data=rdata2d, flag='read', readvar=readvar)
X   call ncd_io(ncid=ncid, varname='LANDMASK', flag='read', data=ldomain%mask, &
       call ncd_io(ncid=ncid, varname='mask', flag='read', data=ldomain%mask, &
X   call ncd_io(ncid=ncid, varname='LANDFRAC', flag='read', data=ldomain%frac, &
       call ncd_io(ncid=ncid, varname='frac', flag='read', data=ldomain%frac, &
?      call ncd_io(ncid=ncidg, varname='GLCMASK', flag='read', data=ldomain%glcmask, &

  subroutine surfrd_get_topo(domain,filename)
    call ncd_io(ncid=ncid, varname='LONGXY', flag='read', data=lonc, &
    call ncd_io(ncid=ncid, varname='LATIXY', flag='read', data=latc, &
?   call ncd_io(ncid=ncid, varname='TOPO', flag='read', data=domain%topo, &

  subroutine surfrd_get_data (ldomain, lfsurdat)
?   call ncd_io(ncid=ncid, varname= 'PFTDATA_MASK', flag='read', data=ldomain%pftm, &
    call ncd_io(ncid=ncid, varname=lon_var, flag='read', data=surfdata_domain%lonc, &
    call ncd_io(ncid=ncid, varname=lat_var, flag='read', data=surfdata_domain%latc, &

  subroutine surfrd_wtxy_special(ncid, ns)
X   call ncd_io(ncid=ncid, varname='PCT_WETLAND', flag='read', data=pctwet, &
X   call ncd_io(ncid=ncid, varname='PCT_LAKE'   , flag='read', data=pctlak, &
X   call ncd_io(ncid=ncid, varname='PCT_GLACIER', flag='read', data=pctgla, &
X   call ncd_io(ncid=ncid, varname='PCT_URBAN'  , flag='read', data=pcturb, &
    if (create_glacier_mec_landunit) then          ! call ncd_io_gs_int2d
?      call ncd_io(ncid=ncid, varname='GLC_MEC', flag='read', data=glc_topomax, &
?      call ncd_io(ncid=ncid, varname='PCT_GLC_MEC', flag='read', data=pctglc_mec, &
?      call ncd_io(ncid=ncid, varname='TOPO_GLC_MEC',  flag='read', data=topoglc_mec, &

  subroutine surfrd_wtxy_veg_all(ncid, ns, pftm)
X   call ncd_io(ncid=ncid, varname='PCT_PFT', flag='read', data=arrayl, &

  subroutine surfrd_wtxy_veg_dgvm()
===================================

* Okay, now cross reference these with the mcrop_dyn.nc file from the iESM
  run...(see ~/PIC/surfdata_360x720_mcrop_dyn.dod, the ncdump -h of the
  mcrop_dyn.nc file in the test run).
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX (see 4/17/2019)
  I see:
    AREA
    LONGXY
    LATIXY
    LANDMASK
    LANDFRAC
    PCT_WETLAND
    PCT_LAKE
    PCT_GLACIER
    PCT_URBAN
    PCT_PFT

  I'm missing:
    GLCMASK
    TOPO
    PFTDATA_MASK
    GLC_MEC
    PCT_GLC_MEC
    TOPO_GLC_MEC

* So, from this it looks like I want to focus the PCT_landtype vars, and
  maybe the landmask and frac, plus the coordinate description
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
! Now, finally, cross reference with what the GCAM stuff in iESM actually
  writes out.  See below at 1/18/19 for more information about how this is
  used.

  It looks like the function calchurtt() calls sethurttcrop(),
  sethurttpasture() and sethurttlanduse() to modify the outhurttpftval[][]
  array, which is then used to write out a new mcrop_dyn.nc file.  Okay!

  Looking at sethurttcrop(), there are a lot of output PFTs being
  generated...

  STOP.  This is all very obfuscated, so it will take some time to track
  down.  But!  I see in all of these cases where we read in

    LANDMASK
    LANDFRAC
    PCT_WETLAND
    PCT_LAKE
    PCT_GLACIER
    PCT_URBAN
    PCT_PFT

  ...via lines c. 5220 in updateannuallanduse.c:

		readlandmask();
		readlandfrac();
		readlakefrac();
		readwetlandfrac();
		readicefrac();
		setvegbarefrac();
		
		/* now read the reference year pft data */
		readcurrentpft();
		readcurrentpftpct(2000);

  I think readcurrentpft() simply maps the pfts to the grid, and
  setvegbarefrac() is simply what fraction is left from each grid cell when
  you subtract ice, lake, and wetland. 

  Okay, so I'm pretty sure what we are writing out has to be these values,
  however they are calculated.
  
4/15/2019

* Jeepus.  I simply can never remember how to do this:

  git clone git@github.com:E3SM-Project/E3SM.git E3SM_active_gcam
  cd E3SM_active_gcam
  git checkout bishtgautam/gcam/add-submodule
  git submodule update --init
  git checkout -b shippert/gcam/active-gcam

4/10/2019

* I need to make a branch off of bishtgautam/gcam/add-submodule to do my
  testing; it may have the configuration mods necessary to run on anvil, as
  well as the gcam submodule and other stuff.

* Dumb reminder: to create a new branch from an old master:

  cd <mdir>
  git pull (very important!)
  git checkout -b <new branch>

  Push up to github
  
  git push origin <new branch>

  See:  https://github.com/Kunena/Kunena-Forum/wiki/Create-a-new-branch-with-git-and-manage-branches

* Because it appears the cime developer group is taking over the automatic
  optional stubbing for siac, until I get their updates I'm going to do
  what Gautam did, and track down the compsets for every unit test and
  simply add an _SIAC after the wav component.  If there is an ESP
  component, then, hmm.

  Initially, it looks like a lot of the unit tests are in teh cam
  config_compsets.xml, so there we go.

* The tests are listed in cime/config/e3sm/tests.py, and e3sm_developer
  inherits lnd and atm developer test lists, so that's a start.  I can run
  down the others, too, once I link their test names with the compsets they
  are running.  I still don't like hammering in a _SIAC, but it seems like
  that's the only way I'm going to verify that siac works - it's the
  automatic adding of siac to the list that was the problem.

* I could go back and add in my auto stuff in case.py, now that we have a
  new branch that might be fully caught up.  That would require *removing*
  this _SIAC nonsense, which is a pain, but actually easier than going the
  other way (also, I did backup to a .dist file, just in case.)

! Interesting!  It appears that while clm and cam config_compsets.xml
  changes to explicitly add _SIAC work, the mods to the otehr
  config_compsets.xml are not enough:

    FAIL ERIO.ne30_g16_rx1.A.anvil_intel RUN time=15
    FAIL ERP_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=1943
    FAIL ERS.f19_g16_rx1.A.anvil_intel RUN time=20
    FAIL ERS.ne30_g16_rx1.A.anvil_intel RUN time=20
    FAIL HOMME_P24.f19_g16_rx1.A.anvil_intel MODEL_BUILD time=101
    FAIL NCK.f19_g16_rx1.A.anvil_intel RUN time=345
    FAIL SEQ.f19_g16.X.anvil_intel RUN time=11
    FAIL SMS_D_Ln5.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=1773
    FAIL SMS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel.cam-outfrq9s RUN time=126
    FAIL SMS.ne30_f19_g16_rx1.A.anvil_intel RUN time=19
    FAIL SMS.ne4_ne4.FC5AV1C-L.anvil_intel.cam-cosplite RUN time=147
    FAIL SMS_P12x2.ne4_oQU240.A_WCYCL1850.anvil_intel.allactive-mach_mods CREATE_NEWCASE
    FAIL SMS_R_Ld5.T42_T42.FSCM5A97.anvil_intel RUN time=11

  These are (some? all?) of the non-clm, non-cam tests listed in
  cime/config/e3sm/tests.py for e3sm_developer.  And it appears they are
  failing with the 'area' and 'aream' AVects, too:

    0: MCT::m_AttrVect::indexRA_:: ERROR--attribute not found: "area" Traceback:  
    0: aa area ->MCT::m_AttrVect::indexRA_
    0: MCT::m_AttrVect::indexRA_:: ERROR--attribute not found: "aream" Traceback:  
    0: aa aream->MCT::m_AttrVect::indexRA_
    ...

  ...from the run log.  Hopefully, the CIME guys will figure that out while
  adding in support for the optional_stubs, but if not I may have to dig
  through again and figure out how default area and aream are set for stub
  components.

  It might be that the cam and clm test cases don't do any actual coupling,
  while these do.  That doesn't sound likely - there are tons of land cases
  in e3sm_land_developer, for one thing, and what are they all testing?  So
  maybe it's something else?

! Also, be careful - I might be crashing out because I'm running too many
  jobs on anvil, and they are stuck "pending" until they are killed or
  something, before actually FAILing the run.  So it might be that clm and
  cam are not working with _SIAC after all.

  I need to wait until tomorrow, then find the command to tell me what jobs
  I have scheduled, and if none are there but I'm still listing some as
  PEND then I need to resubmit.

! Anyway, the 'area' and 'aream' stuff might still be affecting me, but for
  now I'ma wait until the CIME folks get their optional stub stuff ready;
  perhaps some of the stuff Gautam has been pushing up takes care of this
  initialization or something.

4/5/2019

* Trying to work with Gautam's rebased version, in E3SM_gb.  He added _SIAC
  to all the (clm) compsets, but that's because he didn't realize I'd
  modified case.py to have an optional 'siac' stub.  So, in order for the
  non-clm tests to work, I backed out the SIAC tag on all compset files,
  and added case.py back in - but there have been some additional issues to
  deal with from that:

1 E3SM_gb/cime/scripts/lib/CIME/XML/component.py, line 203, it looks like
  we need to add 'iac' to the components that may not have a description
@ For these unit tests, that's probably okay, but we may need to do
  additional work to makes sure we use the description if it *does* exist. 

3/14/2019

* Okay, so I'm not really getting how to run ddt - I can't figure out what
  goes in each element to make it submit the case and connect to it.  When
  I put in the qsub line I get from ./preview_run, it runs the job but it
  never seems to enter the debugger.

  It's possible that I'm using the wrong software in my ~/soft-ddt.sh file,
  which is what we load when we start up a new remote ddt sesssion.  To
  check this, here is a typical compile line from the mct build:

===================
  mpif90  -c  -I. -I../ -DLINUX -DNDEBUG -DMCT_INTERFACE -DHAVE_MPI
  -DFORTRANUNDERSCORE -DNO_R16 -DCPRINTEL -DHAVE_SLASHPROC -DSYSLINUX -DCPR
  -convert big_endian -assume byterecl -ftz -traceback -assume realloc_lhs
  -fp-model source  -O2 -debug minimal  -free
  -I. -I/lcrc/group/acme/shippert/acme_scratch/ERS.f19_g16_rx1.A.anvil_intel.20190312_155150_nt9a2a/bld/intel/mvapich/nodebug/nothreads/include
  -I/lcrc/group/acme/shippert/acme_scratch/ERS.f19_g16_rx1.A.anvil_intel.20190312_155150_nt9a2a/bld/intel/mvapich/nodebug/nothreads/MCT/noesmf/c1a1l1i1o1r1g1w1e1i1/include
  -I/soft/spack-0.9.1/opt/spack/linux-centos6-x86_64/intel-17.0.0/netcdf-4.4.1-gpk22cidfgknxbc6wjuimdkqifhfhg2j/include
  -I/soft/spack-0.9.1/opt/spack/linux-centos6-x86_64/intel-17.0.0/parallel-netcdf-1.7.0-zmjpi4rqpzhvul5o5alk2a2ytgxrrxp6/include
  -I/lcrc/group/acme/shippert/acme_scratch/ERS.f19_g16_rx1.A.anvil_intel.20190312_155150_nt9a2a/bld/intel/mvapich/nodebug/nothreads/include
  -I/blues/gpfs/home/shippert/E3SM_dev/cime/src/share/util
  -I/blues/gpfs/home/shippert/E3SM_dev/cime/src/share/include
  -I/blues/gpfs/home/shippert/E3SM_dev/cime/src/share/RandNum/include
  /blues/gpfs/home/shippert/E3SM_dev/cime/src/externals/mct/mpeu/m_mpif.F90 
====================

* So, I see intel-17.0, netcdf-4.4.1, parallel-netcdf-1.7.0, all built for
  intel-17.  Add in mvapich2, some softenv | grepping, and looking for the
  acme builds...

  +pnetcdf-1.7.0-intel-17.0.0-mvapich2-2.2-acme
  +mvapich2-2.2-intel-17.0.0-acme
  +netcdf-c-4.4.1.1-f77-4.4.4-intel-17.0.1-mvapich2-2.2-parallel-acme


3/13/2019

* So, the goal for today is to get the debugger working and start running
  through one of the test cases.  There was some discussion on the
  confluence page for anvil about this - also, I obviously need to figure
  out the interactive queue.

* Debugging documentation (ddt remotely):

  https://acme-climate.atlassian.net/wiki/spaces/SE/pages/178847811/Debugging+on+blues+with+ddt
  https://www.allinea.com/user-guide/forge/ConnectingtoaRemoteSystem.html

  Some interesting notes abouut how to build and run debugging:

==============================

  1) create a small case, and modify the pelayout (to run on fewer tasks,
  and make a more interesting pelayout, in which coupler pes are disjoint
  from atm pes) 

  /lcrc/project/ACME/iulian/ACME/cime/scripts/create_newcase \
    --case /lcrc/project/ACME/iulian/CASES/SMALL4 --res ne4_ne4 \
    --compset FC5AV1C-L --mach blues

  cd  /lcrc/project/ACME/iulian/CASES/SMALL4

  ./xmlchange --id NTASKS --val 4
  ./xmlchange --id ROOTPE_CPL --val 2
  ./xmlchange --id NTASKS_CPL --val 2 
  ./xmlchange --id NTASKS_ATM --val 2
  ./xmlchange --id STOP_N --val 1
  ./xmlchange --id NTASKS_ESP --val 1
  ./xmlchange --id DEBUG --val TRUE

  ./case.setup

  ./case.build

  verify the launch: 

  ./preview_run

  [iulian@blogin4 SMALL4]$ ./preview_run 
  BATCH SUBMIT:
   case.run -> qsub -l walltime=03:00:00 -A ACME case.run

   MPIRUN: mpiexec -n 4 /lcrc/project/ACME/iulian/acme_scratch/SMALL4/bld/acme.exe >> acme.log.$LID 2>&1

  verify that you can launch the case, with  ./case.submit
==============================
  
* Then you launch ~/allinea/forge/ddt and follow the web page to connect to
  the remote ddt, etc.  I really hope it works, because otherwise I'll have
  to launch from X11.

* Anyway, an ./xmlchange call is how you set stuff up to run in debug
  mode.  There doesn't seem to be an interactive queue, but so far my unit
  tests haven't taken long to start up, so there you go.

* The ddt client download page was annoyingly difficult to find - ultimate
  it was:

  https://developer.arm.com/products/software-development-tools/hpc/downloads/download-arm-forge/older-versions-of-remote-client-for-arm-forge

  Note that "arm" is not my ARM, but allinea something whatever that leads
  to ARM as an acronym for this SW development firm.  Whatever.

* I should look up totalview...huh, we *do* have totalview on blues.  I'm
  going to have to do some googling tomorrow to figure out how to do all
  this debugging stuff, but let's try it with ddt first and see if that
  gets us where we need to go.

3/12/2019

* Okay, the initial model build, at least, fails because of improper mpas
  building, I think - it can't find some mpasli files.  dev.master has no
  problem there, I think, but dev does - and a straight up git submodule
  update --init seems to fail in dev.  So:

  Try to clone and build dev again:

  git clone git@github.com:E3SM-Project/E3SM.git E3SM_dev2
  cd E3SM_dev2
  git checkout shippert/cpl/add-gcam
  git submodule update --init

  ...well, that seems to have worked!  Okay, let's try and do unit testing
  with dev2.

  NOPE - it did not work - we get this error at the end:

----------
fatal: reference is not a tree: acf0ccb6273a950022484bdb33dca1d406101bc6
fatal: reference is not a tree: c35922ee6d866c990f52588d195d9405563fdd80
fatal: reference is not a tree: 1d80ca5d7d434df3d51b1ca68177dd91440eecbd
Unable to checkout 'acf0ccb6273a950022484bdb33dca1d406101bc6' in submodule path 'components/mpas-cice/model'
Unable to checkout 'c35922ee6d866c990f52588d195d9405563fdd80' in submodule path 'components/mpas-o/model'
Unable to checkout '1d80ca5d7d434df3d51b1ca68177dd91440eecbd' in submodule path 'components/mpasli/model'
----------

  I do not know what that might mean.

* So, take two: I'ma copy directly the dev.master mpas stuff over to dev,
  and *then* submit a unit test, again.  Straight up:

  cd ~/E3SM_dev.master/components
  cp -r mpas* ~/E3SM_dev/components/  

* New fails:

============================
  E3sm_dev

    FAIL ERS.f19_g16_rx1.A.anvil_intel RUN time=12
    FAIL ERS.f45_g37_rx1.DTEST.anvil_intel RUN time=23
    FAIL ERS_IOP4c.f19_g16_rx1.A.anvil_intel RUN time=11
    FAIL ERS_IOP4c.ne30_g16_rx1.A.anvil_intel RUN time=12
    FAIL ERS_IOP4p.f19_g16_rx1.A.anvil_intel RUN time=12
    FAIL ERS_IOP4p.ne30_g16_rx1.A.anvil_intel RUN time=10
    FAIL ERS_IOP.f19_g16_rx1.A.anvil_intel RUN time=12
    FAIL ERS_IOP.f45_g37_rx1.DTEST.anvil_intel RUN time=29
    FAIL ERS_IOP.ne30_g16_rx1.A.anvil_intel RUN time=9
    FAIL ERS_Ld5.T62_oQU120.CMPASO-NYF.anvil_intel RUN time=30
*   FAIL ERS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=74
    FAIL ERS.ne30_g16_rx1.A.anvil_intel RUN time=50
    FAIL NCK.f19_g16_rx1.A.anvil_intel RUN time=12
*   FAIL SMS_D_Ln5.ne4_ne4.FC5.anvil_intel RUN time=46
*   FAIL SMS_D_Ln5.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=48
    FAIL SMS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel.cam-outfrq9s RUN time=41
    FAIL SMS.ne30_f19_g16_rx1.A.anvil_intel RUN time=13
    FAIL SMS_R_Ld5.T42_T42.FSCM5A97.anvil_intel RUN time=31
    FAIL SMS.T62_oQU120_ais20.MPAS_LISIO_TEST.anvil_intel RUN time=52
 
============================
  E3SM_dev.master

    FAIL ERS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=42
    FAIL SMS_D_Ln5.ne4_ne4.FC5.anvil_intel RUN time=49
    FAIL SMS_D_Ln5.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=51
!   FAIL SMS.f09_g16_a.IGCLM45_MLI.anvil_intel RUN time=39
!   FAIL SMS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel.cam-outfrq9s RUN time=44

============================

* Okay, well, that's fun - a couple dev.master fails now pass for dev (+
  copied mpas).  I honestly have no idea what is going on there.  Should I
  rerun dev.master?  Could there have been a system issue?

    FAIL ERS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=40
    FAIL SMS_D_Ln5.ne4_ne4.FC5.anvil_intel RUN time=48
    FAIL SMS_D_Ln5.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=51
    FAIL SMS.f09_g16_a.IGCLM45_MLI.anvil_intel RUN time=39
    FAIL SMS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel.cam-outfrq9s RUN time=45

  Okay, a rerun fails the same way.  That's kind of amusing - so my dev
  branch actually got a couple tests to pass where they failed before.

3/11/2019

* I believe the master commit that I branched off of was 'ab81d6e'.

  I get this from this:

  git config --global alias.hist "log --pretty=format:'%h %ad | %s%d [%an]' --graph --date=short"
  git hist

  ...to setup a pretty printing format.  'ab81d6e' was the commit just
  prior to my initial commit.

* So, to get E3SM_dev.master:

  git clone git@github.com:E3SM-Project/E3SM.git E3SM_dev.master
  cd E3SM_dev.master
  git checkout ab81d6e
  git submodule update --init

* That seems to have worked!

* Here are the E3SM_dev.master and E3SM_dev fails for unit tests:

  $ ./cs.* | egrep -i 'fail '

====================================
  E3SM_dev.master

    FAIL ERS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=42
    FAIL SMS_D_Ln5.ne4_ne4.FC5.anvil_intel RUN time=49
    FAIL SMS_D_Ln5.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=51
    FAIL SMS.f09_g16_a.IGCLM45_MLI.anvil_intel RUN time=39
    FAIL SMS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel.cam-outfrq9s RUN time=44
====================================
  E3SM_dev

    FAIL ERS.f09_g16_g.MPASLISIA.anvil_intel MODEL_BUILD time=14
    FAIL ERS.f19_g16_rx1.A.anvil_intel RUN time=53
    FAIL ERS.f45_g37_rx1.DTEST.anvil_intel RUN time=29
    FAIL ERS_IOP4c.f19_g16_rx1.A.anvil_intel RUN time=17
    FAIL ERS_IOP4c.ne30_g16_rx1.A.anvil_intel RUN time=26
    FAIL ERS_IOP4p.f19_g16_rx1.A.anvil_intel RUN time=20
    FAIL ERS_IOP4p.ne30_g16_rx1.A.anvil_intel RUN time=20
    FAIL ERS_IOP.f19_g16_rx1.A.anvil_intel RUN time=18
    FAIL ERS_IOP.f45_g37_rx1.DTEST.anvil_intel RUN time=29
    FAIL ERS_IOP.ne30_g16_rx1.A.anvil_intel RUN time=19
    FAIL ERS_Ld5.T62_oQU120.CMPASO-NYF.anvil_intel MODEL_BUILD time=22
*   FAIL ERS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=53
    FAIL ERS.ne30_g16_rx1.A.anvil_intel RUN time=60
    FAIL NCK.f19_g16_rx1.A.anvil_intel RUN time=20
*   FAIL SMS_D_Ln5.ne4_ne4.FC5.anvil_intel RUN time=65
*   FAIL SMS_D_Ln5.ne4_ne4.FC5AV1C-L.anvil_intel RUN time=83
*   FAIL SMS.f09_g16_a.IGCLM45_MLI.anvil_intel MODEL_BUILD time=756
*   FAIL SMS_Ln9.ne4_ne4.FC5AV1C-L.anvil_intel.cam-outfrq9s RUN time=53
    FAIL SMS.ne30_f19_g16_rx1.A.anvil_intel RUN time=18
    FAIL SMS_R_Ld5.T42_T42.FSCM5A97.anvil_intel RUN time=36
    FAIL SMS.T62_oQU120_ais20.MPAS_LISIO_TEST.anvil_intel MODEL_BUILD time=21


====================================

* So, the fact that we fail at run time for five dev.master runs is not
  great, but at least we have others we can try to debug.  There are 21
  debug fails, out of 36 tests - if I had to guess, I'd say the other 15
  were simple, non-coupled tests.  I should probably check the status log
  of one of them vs. dev.master, too.

  But, ultimately, it might make sense to port up to current E3SM (merge
  with ~/E3SM) before we get finished with unit testing.

* I'm curious about the model_build fails - there's probably something
  interesting to learn there.  Note that one of the dev.master run fails
  is a dev build fail, too.

* So:

1 check model_build fails
2 figure out how to use the new debugger
3 find a coupled test that we are failing, above, and debug that.

  Hopefully, these three steps will get us most of the way to fully debugged.

2/14/2019

* Man, I keep barking up the wrong tree - I thought I found where we used
  the pftdyn file in BiogeophysRestMod.F90, but that's for restarts, not in
  general.  (I'm sure I'll have to modify that, too, except clm in
  ACME/E3SM doesn't have such a file...)

  Anyway, grepping for "surfdata" seems to be closer to what we want - but
  the annoying thing is that the surface data file name is in the namelist,
  so it's really hard to track down and verify I'm looking at the right
  thing from the code.  I keep going back and forth with the sample run
  Kate gave me, but it's like chasing your tail sometimes.  Or, more like
  following three forks of a river at once.

* Anyway, I'm now reviewing main/surfrdMod.F90 to see if that may give me
  some clues as to how the stuff that goes into
  surfdata_360x720_mcrop_dyn.nc is actually used by clm.

2/11/2019

* Okay, that docker class didn't do me any favors.  Trying to recover and
  get back to where I was.  Right now I'm trying to figure out what I need
  to write back out for z->l - i.e. the iac -> lnd fields.  Those are in
  the pftdyn file, which means I need to find the clm code in iESM that
  reads that file so I can tell what is important.

* From there, I need to grab those values at the time we are writing them
  to pft, and also then pass them back up to be stuffed into an MCT-style
  avect. 

* So, first things first - 

1/22/2019

* Okay, so tracking down some details, it looks like the surface landuse
  stuff is written out to the "pftdyn" file, which is called
  surfdata_360x720_mcrop_dyn.nc in updateannuallanduse() (hardcoded).
  It appears to take the surfdata_360x720_mcrop.nc landuse file, which
  gives data only for 2005, and updates it to give yearly values in the
  *_mcrop_dyn.nc version.

  My guess is that clm normally reads the *_mcrop.nc to get 2005 landuse
  values, and then runs some simple extrapolation or parameterization to
  take it to the current run year, and does that instead.  With gcam, we
  instead run our more sophisticated landuse calculation and send that to
  clm via this dyn file.

  So: original: surfdata_360x720_mcrop.nc
        w/gcam: surfdata_360x720_mcrop_dyn.nc

* gcam2emissfile_run_mod() writes out co2flux_iESM_dyn.nc, again hardcoded,
  which seems to contain monthly values of the co2_flux. From the ncdump:

	float CO2_flux(time, lat, lon) ;
		CO2_flux:long_name = "CO2 fossil fuel emission flux" ;
		CO2_flux:units = "1e3 g m-2 s-1" ;
		CO2_flux:_FillValue = -999.f ;
		CO2_flux:standard_name = "tendency_of_atmosphere_mass_content_of_carbon_dioxide_due_to_emission
  
  So, this suggests that this is an extra impulse to co2flux, to be added
  to that from the clm calculation, rather than folded into that.  At
  least, at this point in the iac run it is - maybe iESM folds it in later,
  but right here is the actual z2a_z flux I need to grab.

? (I really do need to see how this gets sent up the chain in iESM, though

* In principle, this is great, we have tracked down where this stuff is
  getting written out, and how it's getting passed to the other mods, which
  lets me simply grab the data at that point and pass it back up the chain
  to iac_run_mod(), which can stuff it into the appropriate coupled
  AVects. 

* Issues:

1 still need to write out files, because gcam/glm use them internally, at
  the very least for its own calculations.  Both pftdyn and dynco2fluxfile
  are read first by their functions, and then written out.

2 How to pass this information back through the calling chain.  At least
  gcam2emiss is in fortran, so it can set module variables or something to
  get back to a place where I build my AVects.  updateannuallanduse() is in
  C, so I probably need to modify the fortran wrapper to get a C structure
  containing the landuse info and reformat it into something I can use to
  build the AVects.

3 how do AVects deal with different shapes of coordinate variables?
  co2flux is on a strict lat,lon grid.  landuse appears to be indexed by
  "column", with the column parameterized to lat,lon values.  I need to
  figure out how MCT handles this kind of thing - at the very least, I'll
  need to pass along the column index -> lat,lon information somehow (in
  it's own AVect?  Are their such things as auxilliary or coordinate
  attribute vectors?  Or does an Attribute Vector structure itself have the
  coordinates and indexing info built in?  Review how clm sends information
  to other components, and see how they handle their column-based indexing
  there.

4 I'm pretty sure this the only way iac interacts with atm, but is there
  any other feedback to lnd other than landuse?

-----------------

* Wrt (3), above, check decompMod.F90 in the clm/src/main directory - it
  defines a way to convert "clumps" back to atmos physics chunks.  There's
  a lot of info on number of clumps, gridcells, columns, pfts, etc, all
  used to define the coordinates of lnd values.  A lot of this stuff has to
  do with the processor decomposition, which is less of an issue for me,
  but since I'm dealing with land style column variables, it seems like I
  should be able to follow what they are doing there.

  So now consider lnd_import_export.F90, which is how we go from
  lnd2atm_vars to l2x() arrays.  It really looks like it's just
  gathering at the start of lnd_export() - from the proc-based 'g' gridcell
  index to the global 'i' based index.

  So it really looks like it can handle a gridcell and/or column based
  coordinate system - at least, the structure of bounds_type seems to argue
  that. 

! Also, check out the sign convention comment:

       ! sign convention is positive downward with 
       ! hierarchy of atm/glc/lnd/rof/ice/ocn.  so water sent from land to rof is positive

? 
?  I'll have to think about that - so co2 from iac to atm is negative?
? 

1/18/2019

* Whew!  We are finally getting somewhere.  Here is the calling tree:

* clm calls iac_run_mod(), which calls, in order, given the appropriate
  alarms:

1 iac2gcam_run_mod()             (clmC alarm)
2 gcam_run_mod()                 (gcam alarm)
3 gcam2emissfile_run_mod()       (as above, plus co2flux and emiss options)
4 gcam2glm_run_mod()             (glm alarm)
5 glm_run_mod()                  ("")

6 glm2iac_run_mod()              (""), calls:
a   updateannuallanduse_main()         calls:
       ...
o      calcchurtt()
o      writepftdynfile() 

* So, finally I see where the file is created that talks back to clm from
  gcam, I believe.

? I don't know if that's *all* that gcam does for clm - update the landuse
  vars - or if there is something else.  But these are the only primary
  functions called by iac_run_mod() - everything else is logging or timing
  or transforming arrays or something like that.  (I *think*).

* Okay, so my belief that the pftdynfile is important comes from a comment
  inside of lnd_comp_mct.F90, line 568: see 1/17/2019, below, bullet 5.

* From updateannuallanduse.c, I see that the pftfile written out by
  writepftdynfil is called: surfdata_360x720_mcrop_dyn.nc.  And we have one
  of those in Kate's test directory:

  /pic/projects/iESM/iesm_model_dir/b.e11.BRCP85C5BPRP.f09_g16.iESM_coupled.001/run

* So, what is apparently happening is that surfdata_360x720_mcrop.nc (no
  _dyn) is an exisiting netcdf file that has information about crop and
? landuse for the year 2005.  I believe that this is used in clm as a
  jumping off point, and is probably the basis of some parameterization or
  simple extrapolation for land and crop use as we go forward.  But iac
  actually *models* land and crop use, so what it does instead is this:

1 take the single-sample surfdata_360x720_mcrop.nc file
2 copy to surfdata_360x720_mcrop_dyn.nc
3 each gcam model year, append new land and crop use information to this
  file 

  So if you look at surfdata_360x720_mcrop_dyn.nc, it has the same fields
  and dimensional shape and everything, except it goes from 2005 to 2092,
  following the TIME coordinate variable (shame on them for capitalizing
  it, but whatever.)  There *are* duplicate values for some TIMEs - for
  example, there are four values with a TIME of 2071.  My guess is
  restarts?  Or bug fix or system barf reruns?  Anyway, it probably appends
  to this _dyn file if it can find it, so any kind of rerunning may lead to
  duplicate days.  The file is huge with lots grid points to work through,
  so it's hard to see if all four 2071 TIME samples are identical or not.
  But that's not really important, so whatever.

* Okay, so I need to figure out what clm uses from this file, and if there
  are any other files that iac modifies that it may use.  So, I need to
  work through updateannuallanduse.c, which is 80 pages of code, with lots
  and lots of comments and sections that says "we don't use this in iESM".

* updateannuallanduse.c is actually a C program, written for some other
  purpose, crammed into iESM - it's actually called via a fortran wrapper
  function by glm2iac_run_mod(), rather than by gcam itself.  But I need to
  figure out what it's doing, and how it is doing it, before I can figure
  out how to make my iac code provide the same information in E3SM.

* It looks like writepftdynfile() mostly copies the
  outhurtpftval[inpft][outgrid] array values into the mcrop_dyn.nc file:
  See c. line 5287.  

  calchurtt() creates outhurttpftval[][], so we need to follow that
  calculation, and maybe some of the ones upstream of it, to extract where
  this information comes from.  Or, not?  Maybe outhurttpftval[pft][grid]
  are the exact values I need to stuff into an attribute vector - instead
  of writepftdynfile() instead call a returnpftdynavect() or something.

* Okay, back to clm - it looks like there is a pftdynMod.F90 module, and
  clm_driver.F90 (as well as other files) all hit on a grep for "pftdyn".
@ So probably should work through clm_driver.F90, figure out how this
  pftdyn file is read in, and that should give me some clues as to how to
  use MCT to set the same information back from iac in E3SM.

1/17/2019

* From the phone call with Kate:

* As it turns out, we *only* use HR and NPP on input - the other
  carbon-related vegetation fields ultimately were not used in this version
  of gcam (I got the impression it didn't accomplish what they want, so it
  probably won't in the future, either).

* Here is an iESM run, which will let me do things like examine the clm.h1
  file etc:

  /pic/projects/iESM/iesm_model_dir/b.e11.BRCP85C5BPRP.f09_g16.iESM_coupled.001

   An example of the outputs, with history files, is available here:
   /pic/projects/iESM/rcp85_results/b.e11.BRCP85C5BPRP.f09_g16.iESM_coupled.001 

* From this, we see that the dimensions are HR[time,column] and
  NPP[time,pft].  This means the only place we get any kind of location
  information is in the HR - column is a 1D parameter of a 2D location (a
@ grid column, obviously).  So I gotta figure out how to make MCT work with
  a parameterized location like this - it can't be all that hard, because
  obviously translations between columns and grid coordinates have to
  happen all the time in a climate model.

* I haven't yet been able to track down how (or what) iac sends back to clm
  in iESM.  It look like the "clm pftdyn file" contains the output from
  iac, as per line 568 in lnd_comp_mct.F90 (in iESM).  I'ma look for it in
  the iESM run above, and see if I can backtrack where it is created - I
  don't really see an nf90_create anywhere else in the whole clm tree in
  iESM.  I might be missing something, or maybe instead it cracks open an
  existing file and updates it?  Kate said something like that, so clm uses
  standard tools to output a history file (thus avoiding naked nf90_create
  calls), and then iac/clm uses nf90_open instead.

  I'm fishing.  My greps for nf90_create and nf90_open haven't been all
  that illumniating on this issue, but maybe I'm just missing it.

* Co2 flux will be separate from lnd for E3SM, but in iESM it might have
  been merged into what lnd sends back.  If so, finding how iESM folds iac
  into clm is gonna be a little more subtle than if it dumped iac results
  to a file or something.  Then again, maybe it's just merged right before
  filling the output attribute vector.  I kind of need to see examples of
  filling in output attvects, anyway, even if it's for iESM.

1/15/2019

* As I've previously encountered, teasing out exactly what information I'm
  supposedt to pull out of clm/lnd and put into an attribute vector is
  excruciating - there are zillions of little calls to nf90_inq_varid()
  floating around.  Some are for writing output (at the end of
  calc_clmC()), some for opening a "base" file, some obviously for cracking
  a history file, etc.  I need to track down what each of these things are
  for. 

  To do this, how about something like this:

  $ egrep 'nf90_open|nf90_inq_varid|subroutine' iac2gcam_mod.F90

  The output below is lightly formatted, to highlight different read
  sections and different subroutines.


==============================


  subroutine iac2gcam_init_mod( EClock, cdata, iaco, gcami)
    status= nf90_open(trim(iac_base_clmfile),nf90_nowrite,ncidbase)
    status = nf90_inq_varid(ncidbase, "abovg_c_mean_pft", base_abovg_c_mean_pftVarId)

    status= nf90_open(trim(clm2gcam_mapfile),nf90_nowrite,ncid)
    status = nf90_inq_varid(ncid, "CCSM_ID", CCSM_IDVarId)
    status = nf90_inq_varid(ncid, "Country_AEZ_ID", Country_AEZ_IDVarId)
    status = nf90_inq_varid(ncid, "GCAM_ID", GCAM_IDVarId)
    status = nf90_inq_varid(ncid, "Weight", WeightVarId)
  end subroutine iac2gcam_init_mod

-----------------------

  subroutine iac2gcam_run_mod( EClock, cdata, iaco, gcami)
    status= nf90_open(trim(iac_base_clmfile),nf90_nowrite,ncidbase)

       status= nf90_open(trim(iac_base_clmfile),nf90_nowrite,ncid)
       status = nf90_inq_varid(ncid,'pft_weight_mean_g',varid)
       status = nf90_inq_varid(ncid,'abovg_c_mean_pft',varid)
       status = nf90_inq_varid(ncid,'blowg_c_mean_pft',varid)
          status = nf90_inq_varid(ncid,'npp_mean_pft',varid)
          status = nf90_inq_varid(ncid,'hr_mean_pft',varid)

    status = nf90_inq_varid(ncidbase, "area", areaVarId)
    status = nf90_inq_varid(ncidbase, "landfrac", landfracVarId)
    status = nf90_inq_varid(ncidbase, trim(var1name), base_var1_mean_pftVarId)
    status = nf90_inq_varid(ncidbase, trim(var2name), base_var2_mean_pftVarId)
    status = nf90_inq_varid(ncidbase, "pft_weight_mean_g", base_pft_weight_mean_gVarId)
  end subroutine iac2gcam_run_mod

------------------

  subroutine calc_clmC(yy,mm,bfn,out_pft_weight,out_abovg_c,out_blowg_c,out_npp,out_hr,calc_avg)

        ! Read restart
        status= nf90_open(filename,nf90_nowrite,ncid)

        status = nf90_inq_varid(ncid,'year',varid)
        status = nf90_inq_varid(ncid,'month',varid)
        status = nf90_inq_varid(ncid,'pft_weight_mean_g',varid)
        status = nf90_inq_varid(ncid,'wcnt',varid)
        status = nf90_inq_varid(ncid,'abovg_c_max_pft',varid)
        status = nf90_inq_varid(ncid,'blowg_c_max_pft',varid)
           status = nf90_inq_varid(ncid,'npp_max_pft',varid)
           status = nf90_inq_varid(ncid,'hr_max_pft',varid)
        status = nf90_inq_varid(ncid,'abovg_c_mean_pft',varid)
        status = nf90_inq_varid(ncid,'blowg_c_mean_pft',varid)
           status = nf90_inq_varid(ncid,'npp_mean_pft',varid)
           status = nf90_inq_varid(ncid,'hr_mean_pft',varid)
        status = nf90_inq_varid(ncid,'cnt3',varid)

     status= nf90_open(trim(filename),nf90_nowrite,ncid)

        ! If first call - so these are coordinates 
        status = nf90_inq_varid(ncid, "lon", varid)
        status = nf90_inq_varid(ncid, "lat", varid)
        status = nf90_inq_varid(ncid, "area", varid)
        status = nf90_inq_varid(ncid, "landfrac", varid)
        status = nf90_inq_varid(ncid, "pfts1d_ixy", varid)
        status = nf90_inq_varid(ncid, "pfts1d_jxy", varid)
        status = nf90_inq_varid(ncid, "pfts1d_itype_veg", varid)
        status = nf90_inq_varid(ncid, "pfts1d_itype_lunit", varid)
        status = nf90_inq_varid(ncid, "cols1d_ixy", varid)
        status = nf90_inq_varid(ncid, "cols1d_jxy", varid)
        status = nf90_inq_varid(ncid, "cols1d_itype_lunit", varid)

     ! Read in always, so these are our state variables
     status = nf90_inq_varid(ncid, "CWDC", varid)
     status = nf90_inq_varid(ncid, "TOTLITC", varid)
     status = nf90_inq_varid(ncid, "TOTSOMC", varid)
        status = nf90_inq_varid(ncid, "HR", varid)
     status = nf90_inq_varid(ncid, "DEADCROOTC", varid)
     status = nf90_inq_varid(ncid, "FROOTC", varid)
     status = nf90_inq_varid(ncid, "LIVECROOTC", varid)
     status = nf90_inq_varid(ncid, "TOTVEGC", varid)
        status = nf90_inq_varid(ncid, "NPP", varid)
     status = nf90_inq_varid(ncid, "pfts1d_wtgcell", varid)

     ! This is the iac_clmC_file, looks like a history file
        status= nf90_create(filename,nf90_clobber,ncid)
        status = nf90_inq_varid(ncid,'lon',varid)
        status = nf90_inq_varid(ncid,'lat',varid)
        status = nf90_inq_varid(ncid,'PFT',varid)
        status = nf90_inq_varid(ncid,'area',varid)
        status = nf90_inq_varid(ncid,'landfrac',varid)
        status = nf90_inq_varid(ncid,'pft_weight_mean_g',varid)
        status = nf90_inq_varid(ncid,'abovg_c_mean_pft',varid)
        status = nf90_inq_varid(ncid,'blowg_c_mean_pft',varid)
	   ! if calc_avg
           status = nf90_inq_varid(ncid,'npp_mean_pft',varid)
           status = nf90_inq_varid(ncid,'hr_mean_pft',varid)

        ! Restart file
        status= nf90_create(filename,nf90_clobber,ncid)
        status = nf90_inq_varid(ncid,'lon',varid)
        status = nf90_inq_varid(ncid,'lat',varid)
        status = nf90_inq_varid(ncid,'PFT',varid)
        status = nf90_inq_varid(ncid,'area',varid)
        status = nf90_inq_varid(ncid,'landfrac',varid)
        status = nf90_inq_varid(ncid,'year',varid)
        status = nf90_inq_varid(ncid,'month',varid)
        status = nf90_inq_varid(ncid,'pft_weight_mean_g',varid)
        status = nf90_inq_varid(ncid,'wcnt',varid)
        status = nf90_inq_varid(ncid,'abovg_c_max_pft',varid)
        status = nf90_inq_varid(ncid,'blowg_c_max_pft',varid)
           status = nf90_inq_varid(ncid,'npp_max_pft',varid)
           status = nf90_inq_varid(ncid,'hr_max_pft',varid)
        status = nf90_inq_varid(ncid,'abovg_c_mean_pft',varid)
        status = nf90_inq_varid(ncid,'blowg_c_mean_pft',varid)
           status = nf90_inq_varid(ncid,'npp_mean_pft',varid)
           status = nf90_inq_varid(ncid,'hr_mean_pft',varid)
        status = nf90_inq_varid(ncid,'cnt3',varid)
  end subroutine calc_clmC
==============================

* Okay, that at least clarifies which calls are for which function, and
  from which file (at least, a way to track it back.)  The time and
  lat/lon grid should be available in another way

1/14/2019

* Gotta transfer some of my scrawlings back to here again.

* Trying to setup the coupling indices, which have names like this:

  index_x2r_Flrl_foo or index_l2x_Sl_bar

  This lets us figure out which element of the attribute vector is which,
  so if gcam is looking for _foo from some input it has the index set.
  Most of this is automatic and internal, you just need to set up the links
  between what field name you want to associate with which index. The way
  that is done is via the seq_fields_x2r_fields stuff in
  ./cime/src/drivers/mct/shr/seq_flds_mod.F90.

* I've looked that over at least twice since I started, and now it's time
  to figure it out.

  It seems like I haven't modified it at all, even though it's a coupler
  function.  Huh - I didn't need it for the stub, although, maybe I do, and
  that's why we've seen run errors (modulo the debugger issues)?  Hmm.

  Anyway, it looks like I need to add in iac and the 'z' component
  everywhere in that mod.

  No, I did mod seq_flds_mod.F90 in October - what did I do?  It looks
  like I added some seq_flds_x2z, z2x _fluxes and _states vars, but did not
  add the component names or anything further on?

! seq_flds_mod.F90

* Okay, working on this - I'm going to assume I have both states and fluxes
  for now, but it's possible I only have states.  This is where I'm weak,
  trying to figure out the actual science of the code and how it relates to
  these variables and stuff, and unfortunately so far I haven't had much
  luck inferring everything from iESM.

  Interestingly, they seem to allow customized coupler fields through the
  namelist mechanism - if I'm reading this right, maybe you could decide
  at run time to add some more info to the coupler?  Interesting, but
  pretty advanced for my purposes....  Still, I'm going to add in the x2z,
  z2x options.

* Next, we define domain coordinates, lat, lon, hgt, area, etc.

! Okay, this is a little hard to get my head around - it *seems* like, in
  this file, seq_flds_mod.F90, we are building up lists of strings that
  describe the various quantities that we need to toss around between
  different components and and the coupler.  So, for example, here are all
  the Surface Latent Heat Fluxes, that need to get sent into the atmosphere
  and ocean components, *from* the ice and lnd components (line c. 1218 of
  seq_flds_mod.F90).

     call seq_flds_add(l2x_fluxes,"Fall_lat")
     call seq_flds_add(xao_fluxes,"Faox_lat")
     call seq_flds_add(i2x_fluxes,"Faii_lat")
     call seq_flds_add(x2a_fluxes,"Faxx_lat")
     call seq_flds_add(x2o_fluxes,"Foxx_lat")

  ...followed by some calls to "metadata_set" to link the names above with
  various attributes like units and longname and what not.

  So, looking at this, here's what I believe these mean: set Fall_lat to
  the latent heat fluxes coming from the lnd component, Faii_lat to the
  latent heat fluxes coming from the ice component, and Faxx_lat and
  Foxx_lat for the latent heat fluxes sent to the atmosphere and ocean
  components.   I think the xao stuff is a little more complicated, having
  to do with some kind of joint calculation between atm and ocn
  components (maybe at the boundaries?).  Anyway, all the Fa stuff will be
  used by the atm component, the Fo will be used by the ocm component.

  Okay, at the top of seq_flds_mod.F90 we what the letters mean:  Faox_
  means "flux between atmos and ocean computed by coupler".  So things like
  Faxx_lat mean "latent heat flux, between atmos and coupler (!) calculated
  by coupler".  I'm afraid I don't understand what flux between a component
  and teh coupler might mean...

  Anyway, there's some merging and scaling rules, but they all seem to be
  associated with atm component, which makes sense - that's global, and
  everything else is not, so the lndfrac, icefrac, ocnfrac scaling issues
  are important there.  For going between lnd and gcam, though, I don't
  think scaling is an issue.

  (Does "scaling" mean something like - if over land, lndfrac = 1,
  otherwise lndfrac=0, and then multiply by lndfrac everywhere?)

* Okay, clearly I need a list of fields that GCAM pulls out of the land
  model and then map those to appropriate Sl_foo states and any fluxes that
  might occur.  I'm thinking it's mostly state variables, since gcam is
  generally only on land and the boundary overlaps aren't important.  But,
  maybe the carbon dioxide output is a Fzaz_co2 variable or something?

* Also, searching for carbon gives some interesting hydrophilic and
  hydrophobic deposition fluxes between ice and ocean, and atmos and
  coupler (!) (Faxa_...).  So "deposition" suggest this is carbon that
  comes out of the first component and into the second - dropping out of
  the air, or melting from the ice.  I still don't understand what "flux
  between atmos and coupler" means, though - does "coupler" mean *global*
  coverage or something?  Or does the coupler do some actual
  calculations with Fmxn_ style fluxes?

* There are also carbon concentrations - So_doc1, So_doc2, e.g. for ocean.

* Then, "sea ice dissolved organic carbon flux", Fioi_doc1 -  okay, I'm
  getting those, that's carbon released into the ocean from sea ice
  melting.

* Now, if flds_co2b is set, we do "Fall_fco2_lnd", "surface upward flux of Co2
  from land".  The flux between the lnd component and the atmos component,
  to represent carbon being released into the atmos from the lnd
  component.  So, this suggest maybe "Fazz_co2" is how I send co2 fluxes
  back from gcam, probably to merge with the values coming from lnd.

  Here is a tricky part, then, scientifically - the lnd component is source
  of carbon up into atm.  Right now, that's just from the ground cover,
  right - trees decaying, that kind of thing.  So gcam needs to account for
  economic (human) development in the same way, but also may change the lnd
  flux - if we cut down trees to make a city, that's less decaying co2 but
  more anthropogenic co2.  This suggests a carbon "flux" between lnd and
  iac, but I'm not sure it's a flux in the way E3SM uses the term.  Maybe
  just a state change?  Does flux mean "go across geographic boundaries
  between the physical domains of the components?"  But what about river
  runoff and stuff like that - any Flrl_ or Flrr_ fluxes?  I'm sure there
  are. 

* Okay, now more complications - we do different things with flds_co2a
  vs. flds_co2b vs. flds_co2c, etc, which are gonna be namelist options.
  These are obviously some kind of different options for different ways of
  modelling co2, and there is some overlad (Fall_fco2_lnd is in both b and
  c, as are the Sa_co2{diag,prog} state variables.

* I'm sure there are other vegetation values, but at least these carbon
  fields give me a jumpoff point to trying to map the iESM fields pulled
  from the history files to something we can get out of the coupler.

1/5/2019

* Making some progress - have a working iac_comp_mct.F90 file, although
  some subfunctions and modules will need to be written.  Built almost
  entirely by copying rof_comp_mct and lnd_comp_mct - I've kept all the
  generic looking parts, but I still dont' completely grok all the
  intricacies of MCT yet, so I might be including some stupid or foolish
  stuff. 

* In particular, my understanding of "domain" calculations are how you
  divvy things up amongst your processors.  GCAM currently is a single proc
  component, but I've kept all that domain infrastructure in case (a) my
  understanding is wrong and domain does something else; (b) we go to
  multiprocessor versions of GCAM someday; and (c) so that the calling
  structure is the same for iac - set up gsmap, setup domain, that kind of
  thing.  I think it's likely that all the gsgrid stuff is required by MCT,
  and there very well may need the dom_z indicator in how MCT calls iac.

* Okay, here's some stuff I need to write, either as part of the IAC
  coupler code or elsewehre:

1 Modules:
@ iac_mod module, element iac (re: RunoffMod)
@ gcam_var module, with lat lon, log, startup, instance, active logicals
  (re: RtmVar)

2 functions, either in above or other modules:
@ gcam_cpl_indeces_set() (re: rtm_cpl_indeces_mod)
@ gcam_mpi_init() (re: RtmSpmd_mod)
@ gcam_var_set() (probably gcam_var module)
@ gcam_init()
@ iac_run_mod() - modification of existing function



12/28/2018

* Modules and functions I need to review, and see where they are in clm and
  whether we need them in iac_comp_mct.F90, iac_run_mct():

* clm_instMod, clm2atm_vars, etc.
  clm_driver, clm_drv
  clm_time_manager
  clm_varctl
  clm_varorb

* seq_cdata_setptrs() gets the cdata from the coupler.  So, how do the
  cdata_z (e.g.) structures get initialized in the first place?  There's a
  legacy seq_cdata_init() function in seq_cdata_mod, but it is apparently
  only used for the data models.  Does the (e.g.) lnd_init_mct() thing set
  up cdata?

* Okay, the component_types, which are named just ike components directly
  (e.g. 'iac', 'lnd') appear to be the overall structures that contain all
  the information.  The accessor to get the "cdata" things is
  component_get_cdata_cC(), which just looks for comp%cdata_cc.  So, for
  example, we need to fill in iac%cdata_cc to find the cdata_z we use in
  the iac_run_mct() and iac_init_mct() etc.  Geez, this goes way back up
  the chain.

  Look at component_mod.F90 in ~/PIC/ACME/cime/src/drivers/mct/main.

  Well, it seems like there's an overall array of components, and the
  cdata_cc stuff just contains communication ids, domain and gsmap
  information.  I'm not sure where that stuff gets generated, though....
  See line 138 of component_mod.F90.

! Sheesh, down the rabbit hole, and it's still slipping away, even after
  all this time.  I guess I'll leave initialization to later, and we'll
  puzzle it back together as we need it.

? My big question is what information cdata_z will have?  Is it just
  mapping and communication, or is field info and what not?  AVects?  Is it
  the same as the cdata structures we see in iESM, or similar, or
  completely different?

12/17/2018 2018-12-17 12:58:47

* Notes from iac2gcam_mod.F90

* subroutine iac2gcam_run_mod() - function to run gcam

12/17/2018

* Quick update on all the unit tests over the weekend:

* ACME.master and ES3M *do* fail some unit tests, and they *seem* to be the
  same ones, so there is something going on with Constance wrt to those
  particular tests.  I need to verify they are failing the same tests, if
  they both run them, and then exclude them from the cases I examine for my
  ACME build.

  A quick examination of .../create_test.out tells you some of this - I
  probably do need to do a quick dump of the TestStats using the cs.*
  scripts to get a solid picture of which tests are telling me something
  interesting about my ACME build.

* Quick review: ACME is my build, ACME.master is the master for my branch
  (hopefully, assuming my git-fu is okay), and E3SM (in ~/test) is the most
  recent grab and build of the main E3SM repository, as of last week.

12/14/2018

* Upon spending all night reflecting on this, I've decided a couple things:

1 I don't need to generate baselines normally, they should be correct from
  the machine file (probably made as part of deriving the machine file in
  the first place).

2 I will run baselines for ACME.master, since it has "(no branch)" and thus
  can't figuring out which baselines to use.  I'm going to put those in
  /pic/scratch/d3a230/acme_baseline.

3 I might then use those same baselines for my ACME branch runs, when I get
  that far, just to compare directly with ACME.master.

* Additional notes:

* I think "baseline" are not actual runs but something to do with comparing
  namelist files; if it fails the baseline it means you modified the
  namelisting somehow.  That's why we see these NLCOMP fails when trying to
  generate baselines into the /pic/project/climate area, where I don't have
  permissions. 

* I need to clean up /pic/scratch/d3a230, so I can find things again.  I'm
  moving everything into "old" directories, and will work with
  /pic/scratch/d2a230/acme, .../acme.master, and .../e3sm subdirectories.

* Huh!  If you give baseline information but not a -g, you have to use -c
  to get them to compare.  Which means, probably, that baselines are all
  optional.  What a waste of time, then.  I'ma run with -c and the same
  acme.master baselines with acme, just in case, but from now on just don't
  use -c, -b, -g, or anything related to baselines, and see if everything
  workds. 

! So, first up:

@ ACME.master: git submodule update --init
@ Run the tests for acme.master, which should work.

  ./create_test -p iesm --test-root /pic/scratch/d3a230/acme.master -g \
     --baseline-root /pic/scratch/d3a230/acme.master/acme_baseline \
     --baseline-name acme_master \
     acme_developer >& ~/ACME.master/create_test.out &

@ Run my acme branch again, using same baselines:

  ./create_test -p iesm --test-root /pic/scratch/d3a230/acme -c \
     --baseline-root /pic/scratch/d3a230/acme.master/acme_baseline \
     --baseline-name acme_master \
     acme_developer >& ~/ACME/create_test.out &

@ Run the e3sm tests, without any baselining

  ./create_test -p iesm --test-root /pic/scratch/d3a230/e3sm \
     e3sm_developer >& ~/test/E3SM/create_test.out &

? Well, when I got back from lunch my connection to the PIC had dumped out,
  and now I can't tell if all these jobs that are pending on the Model
  build are actually doing anything or not, or if the compile got
  interrupted or something.  What a mess - this is what I get for starting
  several of these jobs at once, I can't ps -ef and tell what is going
  on...

* Anyway, jeez, the builds take forever to simply compile - my guess is we
  don't do any kind of parallel build, so I guess it just has to go through
  a lot of work to get there.

! Anyway, the most important thing is that acme.master is FAILING on some
  runs!  I see Seg Faults and stuff!  If that's true, it means that my
  code, built after that, may *also* be failing just for the same reason.
  I'm also seeing build fails on mpas - my conjecture here is that the
  submodule stuff is messed up somehow?  That grabbing the most recent
  version of mpas makes the builds not work right.

* Anyway, everything is up in the air - I don't want to start new builds on
  top of this, so for now I'm going to wait until this evening and check in
  on how everything is progressing.  Assuming the E3SM code (which is very
  new) passes all it's unit tests, then we'll check and see if any more
  progress has been done on the others - if not, we'll have to resubmit
  them, this time one at a time, and hope they can run over the weekend.

* Right now, create_test.out has mod dates:

  Constance[ACME]% ls -al create_test.out
  -rw-r--r-- 1 d3a230 users 34398 Dec 14 12:25 create_test.out
  Constance[ACME]% ls -al ~/ACME.master/create_test.out
  -rw-r--r-- 1 d3a230 users 43782 Dec 14 12:34 /people/d3a230/ACME.master/create_test.out

----------------------------
  Constance[ACME]% tail create_test.out
  Finished SHAREDLIB_BUILD for test SMS_D_Ln5.ne4_ne4.FC5.constance_intel in 3984.677218 seconds (PASS)
  Starting MODEL_BUILD for test SMS_D_Ln5.ne4_ne4.FC5AV1C-L.constance_intel with 4 procs
  Finished MODEL_BUILD for test ERS.f19_g16_rx1.A.constance_intel in 1730.722585 seconds (PASS)
  Starting RUN for test ERS.f19_g16_rx1.A.constance_intel with 1 proc on interactive node and 24 procs on compute nodes
  Finished MODEL_BUILD for test SMS.ne30_f19_g16_rx1.A.constance_intel in 1732.463999 seconds (PASS)
  Starting RUN for test SMS.ne30_f19_g16_rx1.A.constance_intel with 1 proc on interactive node and 24 procs on compute nodes
  Finished RUN for test ERS.f19_g16_rx1.A.constance_intel in 111.358953 seconds (PEND). [COMPLETED 1 of 38]
  Starting MODEL_BUILD for test SMS_D_Ln5.ne4_ne4.FC5.constance_intel with 4 procs
  Finished RUN for test SMS.ne30_f19_g16_rx1.A.constance_intel in 107.099653 seconds (PEND). [COMPLETED 2 of 38]
  Starting MODEL_BUILD for test ERS.f19_g16.I1850CLM45.constance_intel.clm-betr with 4 procs
===========================
  Constance[ACME.master]% tail create_test.out
  Starting MODEL_BUILD for test SMS_D_Ln5.ne4_ne4.FC5.constance_intel with 4 procs
  Finished MODEL_BUILD for test SMS.ne30_f19_g16_rx1.A.constance_intel in 1775.574343 seconds (PASS)
  Starting RUN for test SMS.ne30_f19_g16_rx1.A.constance_intel with 1 proc on interactive node and 24 procs on compute nodes
  Finished RUN for test SMS.ne30_f19_g16_rx1.A.constance_intel in 109.920069 seconds (PEND). [COMPLETED 7 of 38]
  Starting MODEL_BUILD for test SMS_Ln9.ne4_ne4.FC5AV1C-L.constance_intel.cam-outfrq9s with 4 procs
  Finished MODEL_BUILD for test ERS_IOP4c.f19_g16_rx1.A.constance_intel in 1768.789043 seconds (PASS)
  Starting RUN for test ERS_IOP4c.f19_g16_rx1.A.constance_intel with 1 proc on interactive node and 24 procs on compute nodes
  Finished RUN for test ERS_IOP4c.f19_g16_rx1.A.constance_intel in 116.711359 seconds (PEND). [COMPLETED 8 of 38]
  Starting MODEL_BUILD for test ERS_Ln9.ne4_ne4.FC5AV1C-L.constance_intel with 4 procs
  Finished MODEL_BUILD for test ERS.f19_g16.I1850CLM45.constance_intel.clm-betr in 4068.002696 seconds (PASS)
---------------------------

  ...so compare against that to see if any more progress on this has been
  done.

* The reason why I'm worried about it is that my ps -fu d3a230 seems to
  only bring up e3sm activity, but it's hard to tell for sure.  So, wait
  for e3sm to finish running and then review.

* ...okay, a couple hours later, and I'm only seeing updates to the E3SM
  run.  So that means the ACME and ACME.master runs did stall out, for
  whatever reason.  Thus, we need to rerun them, and it takes *many* hours
  to run one of these things, so maybe do one per day or something over the
  weekend.  I'd like to set up a script to do this, but I am likely to use
  the wrong options or whatever, so I'd better check in and do it by hand.
  I may even have to come in to work to do this, since I'm not sure why it
  hung before and what might happen if (e.g.) my home connection went down
  or something.

12/13/2018

* So, I'm trying to revert back to my master branch, in order to run the
  unit tests from that.  So I cloned into ~/ACME.master, because I didn't
  want to mung anything up, and did a "git checkout master", which,
  apparently, didn't do anything.

  Thinking that it's possible that's because my clone points back to ~/ACME
  as my master, I then played around a bit and tried this:

  git checkout remotes/origin/master

  ...which seems to have at least grabbed a version of the code without my
  iac changes.  So now the question is - was this the (old) version of the
  master I originally cloned, or is this possibly the latest version of the
  code?  

  It says I'm currently on "(no branch)", a "headless branch", whatever
  that means.  Since I don't want to actually do any development over here,
  that's fine, but I really wish I understood all this arcane git-fu going
  on. 

* For now, I'm going to work under the assumption that it's the master
  code that I'm working with.  Grepping around in cime/scripts/* for
  "acme_developer" hits, while "e3sm_developer" does not.  That doesn't
  prove anything, but I'm encouraged by the "acme"s floating around and
  lack of "e3sm"s.

* So, I'm going to try running:

  cd ~/ACME.master/scripts
  ./create_test -p iesm -g acme_developer >& ~/ACME.master/create_test.out &

  ... and see what happens.

* Ah, crap, it can't determine the baseline because my branch doesn't make
  sense.  But because I don't know anything about what baselines are or how
  they should be used, using the -b option like create_test says is
  problematic.  

  Okay, run without baseline generation, and hope that somewhere in the
  machine config it points to the right baselines.  So far, it appears to
  be doing something when you run without -g, so that's good news.

* So, the runs that need fates are failing on setupt, because
  /people/d3a230/ACME.master/components/clm/src/external_models/fates/main
  doesn't exist - the fates subdir exists, but nothing else is there.  This
  is consistent with what I found before - somehow fates (along with mpas,
  I think) are checked out as part of setup or building?  Maybe as part of
  baselining? 

? Maybe I should do a git checkout master from ~/ACME, and hope that works?
  Then it will have the branch correct and we can run the baselines from
  that?  I'm reluctant to mess with ~/ACME like that, but I'm running out
  of things I know how to do.

* git log tells you the commit messages, so I'm hoping to roll back to a
  reasonable branch name with a "git checkout <commit>".

  ...nope, that gets me back to (no branch) again.  Damn.  Maybe a git
  clone of the repository, then roll back to this commit?  Sheesh, this is
  a nightmare.

* Okay, try this:

  mkdir ~/test
  cd ~/test
  git clone git@github.com:E3SM-Project/E3SM.git
  cd E3SM
  git submodule update --init

  ...that should be the current build of e3sm.  From there, do the git log,
  find the ACME stuff from way back when, and do that.  

  The submodule stuff is for MPAS and fates and sbetr and whatever, so
  hopefully that will cause those kind of errors to stop happening?

  Anyway, maybe I should run the tests on this, before trying to checkout
  an old commit?  I'm trying to avoid the (no branch) stuff, because taht
  mungs the baselines.

* Okay, a list of things I need to consider:

1 fates,mpas,submodules
2 baseline generation?
3 (no branch), impact on running/generating baseline, using baseline?
4 git merge my branch with latest E3SM?

* I'm going to run the create_test -g command from E3SM, using a testroot
  of /pic/scratch/d3a230/e3sm:

  mkdir -p /pic/scratch/d3a230/e3sm
  cd ~/test/E3SM/cime/scripts
  ./create_test -p iesm --test-root /pic/scratch/d3a230/e3sm -g
  e3sm_developer 

* Failed with NLComp, which appears to be because
  /pic/projects/climate/acme_baselines doesn't exist.  Maybe I need to give
  a baseline directory to generate into?  I'm sure that's it, actually -
  generate baselines using the -b directory, and use that for future tests!

  Right!  So use --baseline-root /pic/scratch/d3a230/es3m/baseline!

* Same thing with my tests using my branch and other stuff.

  Okay, once these runs finish, I'm going to do that:

  mkdir  /pic/scratch/d3a230/es3m/baseline
  ./create_test -p iesm --test-root /pic/scratch/d3a230/e3sm -g \
     --baseline-root /pic/scratch/d3a230/es3m/baseline \     
     e3sm_developer

* Do the same thing with my ACME.master:

  cd ~/ACME.master
  git submodules update --init

  (hopefully, this won't be out of phase with my old old code...)

  mkdir -p /pic/scratch/d3a230/acme.master/baseline
  cd cime/test
  ./create_test -p iesm --test-root /pic/scratch/d3a230/acme.master -g \
     --baseline-root /pic/scratch/d3a230/acme.master/baseline \     
     acme_developer
  

=======================

* Anyway, next up is trying to launch the interactive shell via srun, and
  seeing if that works.  Assuming it doesn't, then these are the things I
  need to check to get debugging working:

0 interactive shell (srun)
0 read testing documentation on atlassian again and again  
1 read totalview documentation for how to run
2 check version of ifort
3 check compiler logs and see if everything is compiled with -g 
4 Get a clean install of the latest version of E3SM, and run the tests.

12/11/18

* I'm having trouble committing my changes to git.  I wanted to just commit
  the rename from components/iac to components/gcam, but that's not
  working, so I'm going to try and commit all my changes.

* Thus, here are the files I changed to get it to compile with the iac
  stub:

        modified:   cime/config/acme/config_files.xml
        modified:   cime/config/acme/machines/Makefile
        modified:   cime/scripts/lib/CIME/case.py
        modified:   cime/src/build_scripts/buildlib.csm_share
        modified:   cime/src/drivers/mct/cime_config/config_component.xml
        modified:   cime/src/drivers/mct/main/cesm_comp_mod.F90
        modified:   cime/src/drivers/mct/main/prep_lnd_mod.F90
        modified:   cime/src/drivers/mct/main/seq_frac_mct.F90
        modified:   cime/src/drivers/mct/main/seq_hist_mod.F90
        modified:   cime/src/drivers/mct/main/seq_rest_mod.F90
        modified:   cime/src/drivers/mct/shr/seq_flds_mod.F90
        modified:   cime/src/drivers/mct/shr/seq_timemgr_mod.F90
        typechange: components/homme/cmake/machineFiles/sandia-srn-sems.cmake
        deleted:    components/homme/utils/cime
        modified:   components/mpas-cice/model (modified content)
        modified:   components/mpas-o/model (modified content)
        modified:   components/mpasli/model (modified content)

  I have no idea whats going on with the mpas or homme stuff here - it
  looks like they get modified by the build process or something.  

* I put the actual diffs in a build.diff file - please don't erase that.

? Huh - well, I had to go into components/mpas*/model directories and do a
  git commit -a -m'Submodule commit' to get the broader ACME commit to
  finally go through.  But that begs the question - what exactly is going
  on with any mpas repository changes?  Seriously, I never modified that at
  all myself - the only thing I can think of (which is insane) is that the
  *build* process, somehow, modified the code.  !  Also, these changes were
  listed as a 'typechange', which, I don't know what that means, but it
  seems to be something like changing a link to a file or something like
  that. 

! Anyway, some of my build and testing issues seemed to be related to
  'mpas', if I remember right.  A 'submodule' is a whole other repository
  for code to be grabbed from, so could it be that 'mpas' got changed and
  pulled as part of the build and test, so I have a new version in my
  working area?  That's completely nuts!  That's just a recipe for code to
  get changed out from under me, without my knowing it!  

  A lot of weird things that I don't understand going on here...

12/4/18

* From the slack, in how to debug:

--------
  I tackled this issue previously as:
  - Compile a single test/case with DEBUG=TRUE
  - Start the job in the interactive queue
  - cd <RUNDIR>
  - Launch totalview and run $EXEROOT/e3sm.exe (edited)
--------

* So, how to start interactive queue?

  https://confluence.pnnl.gov/confluence/display/RC/Using+Debuggers+-+TotalView
  https://confluence.pnnl.gov/confluence/display/RC/Launching+Interactive+Jobs

=========
To launch an interactive job, use the isub command:

    isub -A <''your-account''> -W ''mm'' -N ''nn'' -s <''your-shell''>

Arguments:

  -A project account(required): same as the value on your #SBATCH -A line in a batch job

  -W time in minutes (default 30)

  -N number of nodes (= processor core count/24) (default 2)

  -s shell to use (default your current shell)

  -p partition to use (optional, do not change unless you need to)

  -h print a help message

isub requires that you be running X windows (Xming on Windows), since it opens an X terminal.

For example: to run a 30 minute interactive job in csh on 2 nodes under project constancetest:

    isub -A constancetest -W 30 -N 2 -s csh
Launching an Interactive Shell

You can start an interactive shell as in this example.

    srun -A constancetest -p short --time=45 -I60 --pty -N 2 --ntasks-per-node=24 -u /bin/tcsh

The -I60 means that if the job cannot run within 60 seconds, then the
    system will stop trying to run it. In this example, a 45 minute time
    limit is requested, therefore the short partition is picked, since it
    is generally less busy.
==========

* So: isub -A gcam -W 45 -I60 -N 4

  I need to figure out how many nodes are used in my test case.  Or just
  use four.

11/6/2018

* /pic/scratch/d3a230/ERP_Ld3.f45_f45.ICLM45ED.constance_intel.clm-fates.20181101_094300_i966hg

  ./case.build --clean
  DEBUG=TRUE ./case.build

  Check to see if compiling with -DDEBUG and/or -g or whatever.  If not,
  figure out how to compile that way.

  Okay, that failed, because I'm in the csh - so:

  setenv DEBUG TRUE
  ./case.build

  ...and hope that takes.  If not, have to dig deeper...

* Okay, I'm pretty sure that didn't work the way I wanted it to - I'm
  seeing -DNDEBUG and -O2 and no -g in the compile lines of:

  /pic/scratch/d3a230/csmruns/ERP_Ld3.f45_f45.ICLM45ED.constance_intel.clm-fates.20181101_094300_i966hg/bld/acme.bldlog.181106-145641

* I'm not certain where to go from here - look into the individual make
  files?  There has to be something in case.setup or case.build that makes
  DEBUG happen.

  It appears to be set inside of env_build.xml, and other .xml files, so,
  um, how do those get set?  It seems like, from the timing, that
  env_build.xml is created from case.build.  Jeepus, I'm going to have to
  dig into the CASE tools, aren't I?

10/12/18

* Makefile in, I think,
  /people/d3a230/ACME/cime/config/acme/machines/Makefile

  ...that we have to modify to link with iac.  

  Yup, that did it - add libiac.a in the ULIB thing.

* https://acme-climate.atlassian.net/wiki/spaces/Docs/pages/17006928/Installing+the+ACME+tests

  TL;DR - cd ACME/cime/scripts, ./create_test acme_developer -g.
  (There's something about "wait_for_tests", which, I didn't really get.)

  Future runs of acme_developer tests can go without the -g, we just need a
  baseline for the initial run.

  Should probably dump output to a .out file.

10/11/18

* Almost - but I'm not linking with the iac library I build, so we fail on
  the linking part.  Look at:

  ~/ACME/cime/src/drivers/mct/cime_config/buildexe

  ...to see if you can figure out where I'm missing something.

10/10/18

* Finally tracked down all the .xml changes I need to make, created a siac
  stub, and did other misc changes.  I need to document what these file
  changes are - the .xml files, at least, are the configuration changes
  needed to add a new component class.

@ Fractions - I need to go back to seq_frac_mct.F90 and update the
  fractions infrastructure for IAC.  Right now, I've passed in the argument
  and declared everything, because I'm working on compilation right now,
  but eventually we need to understand what exactly is going on here.  It
  seems like each component has it's own fractions_zx built, but I can't
  quite figure out what it is doing and what would be appropriate for the
  iac to use.  For now we simply do nothing, so fractions_zx will be
  allocated but unused - I'll have to see later on if that mungs up the
  interpolation step before we call iac.

* Fractions are the fraction of each grid cell that is lnd, ocn, ice, and
  atm.  Frac of atm is always 1.0, which makes sense - you have an
  atmosphere everywhere on the planet.  Fractions of lnd is static, but the
  fractions of ice change over time, which is why this is important, I
  think - you need a dynamic update of surface type fractions, so you have
  to rebuild these fractions every Nth iteration and you have to use these
  fractions instead of some kind of default mapping.

  But!  Does GCAM use them at all?  Do we need them?  I feel like any time
  we use data on a grid we need to have the fractions available, even if
  our couplings (lnd and atm) will never change.

? Something else - writing history and restart files.  We need to figure
  out if that makes sense for iac and what should be written - it may be
  that the GCAM outputs are fine for what we want and the total history
  files aren't needed.  Restart is a little different question, if we have
  a persistent state inside of gcam that we need to track.

* List of things I need to revisit as I integrate iac/gcam:

1 fractions_zx
2 seq_rest_read and seq_rest_write - figure out what to dump to a restart
  file
3 seq_hist_write, seq_hist_writeavg

10/8/18

* Finally tracked down (I think) the way to add a brand new optional
  component to the build system.  The issue is that the existing 7
  components are pretty hardwired into seem, and we don't want to require
  IAC in the case string if we aren't building with it (which would require
  retrofactoring every test case string).  Apparently, the ESP component
  was also added after with the same issues, so it was made optional,
  giving me a path forward in IAC.

  There's probably more to this, but this is what I've done so far:

1 Modified ~/PIC/ACME/cime/scripts/lib/CIME/case.py c. L526.
  The way ESP was optional is that we check the number of allowed
  components vs. the number specified on the case string, and if we had
  fewer than expected we added 'sesp' (the stub ESP) to the build.  
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
  So, do
  the same thing - if we are short, look for ESP and, separately, IAC, and
  add the default (stub) if they are missing.  (For now, I'm adding the
  actual iac build, because I want to test and I haven't written a 'siac'
  build yet.)
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

! I THINK THIS IS NOT RIGHT - I *believe* the order of things matters - we
  need the fifth component class to match the fifth component, so we can
  map one to the other.  That's insane - we should have a mapping in the
  build, right?  Maybe that's what the case string is for, though, to say
  "first use this for coupler, then this for atm, etc...

  ALSO, "component class" is 'iac', while "component" is 'gcam'.  Review rules
  for case string and see if order matters!

  ...this means that we might have to require IAC to be either after or
  before ESP in our case string.  Probably before, to make cases that work
  for ESP correct.

* Okay, so some clarity - the order DOES matter, so we pretty much have to
  tack IAC on to the end, and have to use the count of components to figure
  out if we have to add ESP and IAC or not.  And, I'm pretty sure we will
  have to eventually require explicitly an ESP component if we want to use
  IAC. 

  So, if 2 less, use SESP and SIAC.  If 1 less, add SIAC.

* Also, compsets are defined in config_compsets.xml, but these are all over
  the place - in the mct area, and in a lot of component areas.  This is
  where the strings are built, so probably an IAC config_compsets.xml is
  the way to build our gcam runs.

* Finally, the case strings *do* give the model - for example, in
  .../components/cam/cime_config/config_compsets.xml, we define the F1850
  compset with this string:

  1850_CAM4_CLM40%SP_CICE%PRES_DOCN%DOM_SROF_SGLC_SWAV

  So, use cam4, clm40, and cice models, then the data ocean model and stub
  rof, glc, and wav.

* So, this explains why we have to have a 'siac' - it's in the place of a
  real component, because we can't consider iac and gcam as the same thing
  - we may eventually want to run another economic model as the iac
  component (or another version of gcam).

* I know other versions of our models are used - cam4 and cam5, clm40 and
  clm45.  So somewhere in the build system must be a way to map the "CAM4"
  string into telling it to compile cam4 vs. cam5.

  My guess is it will show up in the buildlib script somewhere...

* Okay, this ALSO means that I need to modify the buildlib for
  mct/coupler/whatever - that's the one that decides to build the
  iac_foo.mod stuff I coded up.  That's the key one to get the default
  tests up and running.

2 Modified .../cime/src/drivers/mct/cime_config/config_component.xml,
  adding IAC to the COMP_CLASSES.  That will get my component_classes right
  for the above check to work.

3 buildnml
  buildlib for iac component (not yet finished)

  Hopefully, the above config changes will automatically send it into the
  iac component directory and run these scripts to build and create the
  namelists.  But we probably have some more work to do to force it to link
  or install correctly - hopefully not.

  Update: yes, there is more work.  Only the coupler understands "iac" - we
  have to somewhere tell it to build gcam when we want the iac component.
  In this way, we can swap out 

* Everybody uses perl for buildnml; only clm uses python for buildlib.
  That's almost certainly no longer true with recent branches, which is an
  issue, because I should use python... (they are porting to python).

  

7/2/18

@Xseq_com_mct.F90 mods are done - mostly cut and paste and mod to iac, IAC,
  and 'z'.  It mostly is just counting up and assigning indeces to the
  procs and stuff, so it's all the same for each component.

? Next up is figuring out how to set an iac component type.

6/27/18

* Modifications to seq_infodata_mod.F90:

* mostly just search for "lnd", and wherever you see variable definitions
  or function arguments that have that, you'll see similar sections for
  every component, so add an "iac" version.

! But!  Down in the function seq_infodata_Exchange() there's a little more
  to it - that function is used to broadcast some of infodata between pes
  to exchange information (really, just make it global).  But I need to
  analyze that function first to see what is involve - I could just mimic
  rof or wav or somethign like that, but I'd rather get a sense of what
  this function is doing wrt to what I think iac should be doing.

  ...okay, it looks like just an interface for broadcasting the simple
  infodata info like "lnd_present" and whatever.  Good, I can just copy
  it. 

* Okay, seq_infodata_mod.F90 all done!

* The final thing is seq_comm_mct.F90, I think, before I'm ready for the
  initial commit.

  (okay, I committed without setting seq_comm_mct.F90.)

! Need to track down component type.

@Xcomponent_type_mod.F90
  
  Trivial!  Just declare the iac(:) type and make sure num_inst_iac is
  available, plus there was one  public thing.

@Xcomponent_mod.F90

  Also, pretty trivial - some "one letter" stuff to add in 'z' for iac, and
  that's it.  But, this is where component_run() is, so it's worth looking
  that function over (and adding a 'z' one-letterism to it).  Also,
  component_exch() and component_diag(), which are things I need to build
  hooks for, eventually.

? There is some stuff in component_mod.F90 about "aream", in domains where
  appropriate.  That includes most of the domains, apparently, but since I
  don't understand waht it does yet I'ma going to leave out any iac
  additions there.  But if it's something abouut "area matching", which it
  might be because there's a lot of "samegrid_xy" stuff, then maybe it
  makes sense to keep iac on the lnd grid and this is the infrastructure
  for doing that?

  I should review prep_lnd_get_mapper_Sa2l(), which suggests it's something
  like mapping state variables from atmos to lnd or something.

6/25/18

* Some random notes on things I've been wondering about:

* xxx_prognostic means "does model xxx need input from the driver",
  according to the description in seq_infodata_mod.F90.   I suspect this is
  to allow the model to run several time steps while only interacting with
  the driver some of them - once per day, etc.  This means xxx_prognostic
  is probably set at the top of every driver loop.

* xxx_present means "does xxx component actually exist", so it's set at run
  time. 

? My current question I'm pondering is what do we do when we are in the
  coupler, the lnd (e.g.) model is running, so it does the standard
  prep_lnd_calc_z2x_lx() call to set up getting the inputs from iac ('z')
  to lnd ('l')...but it's not the one time a year in which the iac is run?
  What does the z2x_lx(:) Avect hold when the model hasn't run this loop?
  What does the lnd model do with that information?  Does
  prep_lnd_calc_z2x_lx() have some kind of interface or alarm for
  determining that iac has run now and now we need to apply its output this
  run, or do we assume we are going to use the same iac outputs for every
  sample of the upcoming year, or what?

* There's a lot of stuff in seq_infodata_mod.F90 that comes from the
  namelist or is set in some other way, so it's going to take me a bit to
  track down all the various ways adding iac to this mod will matter.

5/23/18

* I'm way far behind, because of personal stuff, but we want to be ready
  with some kind of initial checkin for the code review on July 1.  And we
  don't want to dump a whole garbage can full of code modifications all at
  once and have them tell me I'm doing it wrong.

  So, my initial checkin will be modifications mostly to the coupler code
  to have IAC sections in there - stubbing out the actual calls to setup
  and run GCAM, but getting all the infrastructure together, possibly with
  an initial version of the GCAM code at least in its directory.

  This will let code reviewers see what I'm planning on doing, and also
  might allow somebody to look it over and help me figure out some of the
  problems I've been having.

1/5/18

* Here is my checklist and working notes as I continue the port.  I'm
  losing track of all the new variables and mods and files I need to change
  for each thing I add, so I need a place to scratch them down and mark
  them off.

* cesm_comp_mod.F90:

@ Create iac_comp_mct:
  iac_init_mct
  iac_run_mct
  iac_final_mct

@ modify seq_comm_mct:
  IACID
  ALLIACID
  CPLALLIACID
  CPLIACID
  num_inst_iac

@ modify seq_timemgr_mod:
  seq_timemgr_alarm_iacrun  

? seq_diag_mct:
  seq_diag_iac_mct, maybe more - review what diagnostics we might need

@ seq_flds_mod: (Iac one digit is 'z', because the next best thing to being
  right is to be very wrong):

  seq_flds_z2x_fluxes 
  seq_flds_x2z_fluxes

  Actually, I'm not sure if we are using fluxes or scalars or whatever.
  I'll have to figure that out.

@ component_type_mod:

  iac - component

@ prep_iac_mod:

  Whole thing, to prep the iac component.

? At this point, I'm not sure if we need the prep routines as listed in
  line 197-204.  Revisit.

? Don't know what fractions_?x(:) arrays do, but I better stick one in on
  line 223:  fractions_zx(:)

@ c. Line 272: Figure out how to set and use iacrun_alarm and EClock_z

? c. Line 365: iac_prognostic - "iac component expects input", which, okay,
  but I'm not sure what this logical means.  Does it get set somewhere as
  part of the alarm mechanism?

? L418: Don't know what "iac_gnam" means.

* L431: samegrid_zl - my belief is that gcam is set up to take data on the
  land grid and convert to regions from there; hence, I'm adding a
  samegrid_zl logcial, because there's a bunch of those around.  But I
  don't really know when it is used or how it is set...

? It appears to be used for SCM, so the question is, do we need to care
  about IAC and SCM?

? L445 et al - do we want to create history files wiht iac?  For now, say
  no.

? L536+ - I guess I'll add in mpicomp ids and "iamin" for IAC, but who
  knows how they are really used.

? L588: "component instance counters"?  Okay, ezi.

* cesm_pre_init1():

! L662: Finally, some coding.  I need to follow suit with these, as they
  set up all the communication stuff.  Make sure we have defined all these
  approporiate seq_comm_name and seq_comm_iamin arrays.

* cesm_pre_init2():

* L930: call to seq_infodata_GetData() to find iac_present, etc.  So I have
  to make sure the right stuff is in the infodata structure, too.

* L1128: iac_phase in seq_infodata_putData().  Hurm.

? L1146: we now get to some specials about single_column modelling on a
  non-aqua planet.  Leave them alone for now.

* cesm_init():

  Awesome - we have two pre-init functions before we finally init!

@ L1215+
  component_init_pre()
  component_init_cc()
  component_init_cx()  

  ...all called with iac, but probably generically.  Check, though.

@ L1342+
  component_get_iamin_compid()
  component_get_name()

@ L1369: Review seq_infodata_exchange()

  Again, lots of these are called generically and set up to allow
  components to talk to each other.  But it's still pretty opaque.

@ Coupling flags: L1508.  I'm just faking it here, and maybe I need to
  check on prognostic...

  I *think* we may need to do prognostic here - the comments at the top 
  sketchily suggest that "prognostic" means "expect input", which is what
  lnd and atm need from iac.  The question is - do we need an
  iac_prognostic to to couple lnd_c2_iac?

? L1691: prognostic instances, and making sure num_inst_xxx =
  num_inst_max.  I really don't understand that, but I'll need to figure
  out the prognostic stuff, first.

* L1719: prep_iac_init() - this is obviously something I have to write.

? L1765: seq_domain_check() - I suspect this won't be needed for iac, but
  check into it nonetheless.  I'm not sure what "domains" are in this
  section, and there are some component-based elements in the call to
  seq_domain_check(), but they seem linked to the "samegrid_xx" variables. 

? L1813: compnent_init_areacor(): some kind of area corrections that I
  don't understand.  In this case, they do seem to call for every
  component, so add it in and figure it out later.

? L1860: component_diag(), something about "recv IC xxx", which I don't
  understand either.  This probably means I need to develop diagnostics for
  iac. 

* L1890: more about fractions, which I still don't get.

@ L1894: seq_frac_init() - modify for iac and fractions_zx.

? L1980+ Okay, we are starting to get into initialization and prep for
  model runs, with a lot of specific component related elements.   Look for
  lnd_ and atm_ prep for coupling with other components...

* L2043: this component_exch(atm, flow='x2c',...) - This is an init, so
  we haven't looped over timesteps yet, so maybe it doesn't need to get
  anything out of the iac output yet.  But this is the kind of thing we are
  looking at.

* seq_flds_x2a_fluxes - this is the array (I think) of fields that we need
  to grab from the coupler into the atm model.  So I will have to modify
  this somewhere to include feedback from iac.

? L2109+ - once again, figure out what kind of thing is happening with
  these calls to prep_lnd_calc_r2x_lx() et al.  They describe it as mapping
  initial r2x_rx and g2x_gx to _ox, _ix, and _lx, which I guess is some
  kind of grid mapping from the glc and rof components to ocean, ice, and
  lnd.  I'm hoping this kind of thing doesn't matter for iac - right now we
  expect the lnd mapping on input to gcam, and hopefully the lnd and atm
  mapping are the same.  Otherwise, maybe see how we do land to atm
  mapping, and make sure the gcam outputs go along with that?

? seq_hist_write() - add in iac and fractions_zx.

! Finally, cesm_run(), which is I think something I know a little about.
  Let's find out how wrong I am!

@ L2167: seq_comm_mct.mod.F90, iac_layout

* L2171: hashcnt - I have notes on this somewhere, but I still don't know
  if it applies to iac or not.  We'll have to revisit this.

? L2193: Do I need to set iac_phase=1?  We don't have wav or rof, and iac
  is less connected than anything, so why don't I skip this for now.

@ L2252: define iacrun_alarm somewhere, seq_timmgr_alarmIsOn(),
  seq_timemgr_alarm_iacrun.  This hopefully will make it clearer how alarms
  go and when to run things.  Also, maybe review glcrun_avg_alarm and
  ocnnext_alarm and see why they need extra alarms for those comps.

@ L2542: prep_lnd_calc_z2x_lx() - need to write this function, to do the
  iac preparations for updating the land model.  I need to do something
  like this for atm prep, as well.

? 
? The Big Quesiton I have so far is how this works with the fact that we
? run IAC yearly or five yearly, not on the same time grid as the lnd and
  atm models.  Presumably, prep_lnd_calc_z2x_lx() and
  prep_atm_calc_z2x_lx() will simply do nothing except on the right time
  scale, but it's not clear to me how that works.  Do any other components
  have this delayed and/or long term application?

! OKay, as I understand it, we want to run IAC at the start of a given
  year (or 5-year block), which finally uses the averaged inputs from the
  previous year, and then use that input to modify atm and lnd.  So, if my
  understanding is right, that means we RUN iac before we SETUP lnd and
  atm.  Is that right?  Do we do a full setup/run/post on IAC before doing
  anything else?  That suggests we should put the IAC stuff right at the
  top.

  Let me review other modules and see if I can see that - right now it
  seems like we do setup (all comps), run (all comps), post (all comps),
  but that could be just because they are all on the same time scale.

  Alternatively, I guess, we could run iac at the end, and that post will
  then be available to the next one.  But don't do this - let's run right
  at the top.

  I'm pretty sure it's okay - we apparently have a lot of options for when
  ocn/atm models are run and how they are set up, and they happen spread
! throughout this function.  So, my guess is that I will have to couple in
  iac in a couple different places - stick with the lnd stuff; whereever we
  see lnd_c2_atm then activate an iac_c2_atm as well.

! 
! Okay, I think I'm gonna revise my plan, and just implement iac just like
! all the other components - serial in setup, then use the same parallel
  mechanism and barriers as other components.  I believe what this means is
  that we'll have the previous one year of lnd as input to iac, and then
  the resulting output of the iac will apply on the *second* timestep of
  the year for lnd and cam.  I don't think this is a gross violation of the
  methodolgy, and allows GCAM to be run fully in parallel like other
  components. 

  It's probably not a big deal either way, as GCAM is not computationally
  intensive and runs once a year anyway, but if I ran GCAM serially in the
  coupler before running everybody else's setup (which is essentially how
  it runs in iESM) it would make this component different than the others,
  which is harder to maintain and, crucially, more likely for me to get
  wrong.  Also, who is to say we won't have a CPU intensive IAC module to
  hook in some point in the future?

! l. 2349 - iac_prognostic - I'm not sure what this means, but I think it
  suggests that if we are providing input to IAC this is the section where
  we build those inputs.  When would iac_prognostic NOT be on?  I need to
  ask Kate about this, but for now I'm keeping the form that the other
  modules use.  (I'm following ROF as a template).

? Could iac_prognostic be variable, so it's set only on time steps where we
  need to do stuff?  Hmm.

* prep_iac_accum_avg() - this function should take the average of the accum
  vars.  I know this one.

* prep_iac_calc_l2r_rx() - I still don't know what fractions_Xx suggest,
  but thi sis obviously the function that grabs the lnd vars out of the
  coupler and makes them availalbe (or ready?) for iac to use.

! l. 2361 - prep_iac_mrg() - review what it means to "merge" in this
  context.  Is this just prep work for the diagnostic in the next line, or
@ is this more setup for iac to use?  Review what ROF merge does.

! l. 2376 - figure out how component_exch() works, what it does and what
  all the arguments are.  I'm not sure the timer/barrier format string, but maybe
  it doesn't matter that much.

* l.2772 - the run call is straightforward, but I still am not sure whether
  "_fluxes" is what we are sending or not.  I need to know the difference
  between a flux and state variable, since they seem to be treated
  differently.  

* l.2849 - Just to be cheeky, I'm putting the IAC RECV before everybody
  else.  I'm 78% sure it doesn't really matter, as everything is in
  parallel, but just in case it does I want to run at the top so the lnd
  can use it later.  Also, this WILL matter if at some point we decide to
  simply run in the coupler before doing anything else, or something like
  that (although I think at that point we'll move all these things together
  in one block).

@ l.2878+ - prep_lnd_calc_z2x_lx() and prep_atm_calc_z2x_ax() - functions
  in the land and atm components that I have to write to pull the important
  outputs from iac.  This might be the toughest thing, because I'm not at
  all conversent in those other models and do not know how they use these
  inputs.  I hope the iESM code helps with regard to this, but since it
  couples differently I'm not at all sure that will be the case.

  (Also, make sure these functions are named correctly - I'm still a little
  shaky on the naming arcana).

XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
XXX See below, after seq_hist_write()
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
! l.2895_ - not sure what do_hist_z2x might imply - writing out history
  files?  Just follow the template and figure it out later.  It may mean
  figuring out seq_hist_writeaux() to add in the var names or something.

  (especially with regard to nx, ny, the write_now argument?  I set
  write_now to the t1yr_alarm, because we run yearly, but, yes, well, hmm.) 
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

@ l.3779 - restart file - seq_rest_write() - another in the endless string
  of functions that I hope are easy to modify to deal with an iac
  component.  Add in fractions_zx and the iac component, and hope for the
  best.  

@ l.3803 - seq_hist_write() - same thing.
  l.3812 - seq_hist_write_avg()

? Huh, down here is where some do_hist_a2x stuff happens.  Also the 1 year
  lnd writes; maybe move iac down here?  Do we even need iac hist writes?

! You know what? Screw the do_hist_z2x stuff.  We'll add it in later if we
  want it,  maybe right here, maybe after iac post like rof does.

* Restart -
  l.3953 - seq_rest_read()

* l.4032 - just follow the pattern - see if root, whatever.

! Whoo-hoo, done with cesm_run - now cesm_final():

* l. 4129 - component_final() - review.  It might be part of what you do to
  create a component in the first place.

* Hey, that's it. For cesm_comp_mod.F90.

6/27/17

  Script to run the model, from Balwinder:

  /people/sing201/runscr/int/acme/acme_def_cime5_03022017/acme_def_cime5_03022017
